# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for
# license information.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------
# ---------------------------------------------------------
from pathlib import Path
from tempfile import TemporaryDirectory
from unittest.mock import patch

import pydash
import pytest
import yaml
from azext_mlv2.tests.scenario_test_helper import MLBaseScenarioTest
from azext_mlv2.tests.util import assert_same
from marshmallow.exceptions import ValidationError

from azure.ai.ml.constants._common import (
    AZUREML_PRIVATE_FEATURES_ENV_VAR,
    CURATED_ENV_PREFIX,
    MAX_LIST_CLI_RESULTS,
    REGISTRY_URI_FORMAT,
)
from azure.ai.ml.dsl._utils import environment_variable_overwrite
from azure.ai.ml.exceptions import UserErrorException

from ..util import private_flag


@pytest.fixture()
def enable_private_preview_features():
    with environment_variable_overwrite(AZUREML_PRIVATE_FEATURES_ENV_VAR, "true"):
        yield


@pytest.fixture()
def disable_private_preview_features():
    with environment_variable_overwrite(AZUREML_PRIVATE_FEATURES_ENV_VAR, "false"):
        yield


@pytest.mark.usefixtures("enable_private_preview_features")
class JobScenarioTest(MLBaseScenarioTest):
    @pytest.mark.skip(reason="TODO: CannotOverwriteExistingCassetteException in pipeline.")
    def test_job_command_job(self) -> None:
        # TODO: Assumes there is a compute called testCompute, which may prevent a successful recording
        job_name_suffix = "-1"
        # Updating dictionary with the job name
        self.kwargs["command_job_name1"] = "{}{}".format(self.kwargs.get("commandJobName", None), job_name_suffix)
        create_job = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/command_job/command_job_test.yml --name {command_job_name1} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job)

        # show_job = "az ml job show --name {command_job_name1} -g testrg -w testworkspace"
        # job_obj = self.cmd(show_job)

        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == self.kwargs.get("command_job_name1", None)
        assert "testCompute" in job_obj["compute"]
        assert "AzureML-sklearn" in job_obj["environment"]
        assert "1" in job_obj["environment"]
        assert job_obj["experiment_name"] == "mfe-test1"
        assert job_obj["command"] == "pip freeze"

        show_job = "az ml job show --name {command_job_name1} -g testrg -w testworkspace"
        job_show_obj = self.cmd(show_job)
        job_show_obj = yaml.safe_load(job_show_obj.output)
        assert_same(
            job_obj,
            job_show_obj,
            filter=[
                "status",
                "creation_context",
                "properties.ProcessStatusFile",
                "properties.ProcessInfoFile",
                "tags._aml_system_ComputeTargetStatus",
            ],
        )
        # TODO: why does creation_context differs in the output of creation and show?
        # Ignoring properties ProcessStatusFile and ProcessInfoFile, present only after the job is running

        stream_job = "az ml job stream --name {command_job_name1} -g testrg -w testworkspace"
        self.cmd(stream_job)

        with TemporaryDirectory() as tmp_path:
            root = Path(tmp_path)

            download_job = "az ml job download --name {command_job_name1} --download-path '" + tmp_path + "' -g testrg -w testworkspace"
            print(download_job)
            self.cmd(download_job)
            std_log_file = root / "artifacts/user_logs/std_log.txt"
            assert std_log_file.exists()

        import json

        print(json.dumps(job_show_obj, indent=4))
        updated_job_command = "az ml job update --name {command_job_name1} --set description=bla -g testrg -w testworkspace"
        updated_job_obj = self.cmd(updated_job_command)
        updated_job = yaml.safe_load(updated_job_obj.output)

        assert updated_job["description"] == "bla"
        assert_same(
            job_show_obj,
            updated_job,
            filter=[
                "status",
                "tags",
                "creation_context",
                "properties.ProcessStatusFile",
                "properties.ProcessInfoFile",
                "description",
            ],
        )
        cancel_job = "az ml job cancel --name {command_job_name1} -g testrg -w testworkspace"
        self.cmd(cancel_job)
        # Delete a key regardless of whether it is in the dictionary for the new name
        self.kwargs.pop("command_job_name1", None)
        # Ignoring properties ProcessStatusFile and ProcessInfoFile, present only after the job is running

    @pytest.mark.skip(reason="ServiceError:Service 'DATA_CAPABILITY' returned capability start response with code")
    def test_job_command_job_show_to_asset_create(self) -> None:
        job_name_suffix = "-2"
        self.kwargs["command_job_name2"] = "{}{}".format(self.kwargs.get("commandJobName", None), job_name_suffix)
        create_job_command = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/command_job/command_job_output.yml --name {command_job_name2} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)

        show_job_command = "az ml job show --name {command_job_name2} -g testrg -w testworkspace"
        job_obj = self.cmd(show_job_command)

        job_obj = yaml.safe_load(job_obj.output)

        show_job_stream_command = "az ml job stream --name {command_job_name2} -g testrg -w testworkspace"
        self.cmd(show_job_stream_command)
        show_job_command = "az ml job show --name {command_job_name2} -g testrg -w testworkspace"
        job_out_obj = self.cmd(show_job_command)
        job_out_obj = yaml.safe_load(job_out_obj.output)
        job_output_uri = job_out_obj["outputs"]["default"]["path"]
        # "--model-uri" is an invalid parameter for command "ml model create".
        # model_obj = self.cmd(f"az ml model create --model-uri {job_output_uri}")
        # model_obj = yaml.safe_load(model_obj.output)
        # assert job_output_uri.split("ExperimentRun")[-1] in model_obj.get("path")
        # Delete a key regardless of whether it is in the dictionary for the new name
        self.kwargs.pop("command_job_name2", None)

    #@pytest.mark.public_preview_only
    # This test is not working. TODO: https://dev.azure.com/msdata/Vienna/_workitems/edit/3372868
    @pytest.mark.skip(reason="Recording and replay not working.")
    def test_job_command_job_list(self) -> None:
        """
        To record this test, please prepare a fresh workspace with no jobs.
        Otherwise, the list is going to take a while to return the result.
        """
        job_name_suffix = "jobNameNumber_"
        total_jobs = 4
        for i in range(total_jobs):
            # self.cmd(f"az ml job delete --name {job_name_suffix}{i} -g testrg -w testworkspace")
            self.cmd(
                f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/command_job/command_job_test.yml --name {job_name_suffix}{i} -g testrg -w testworkspace"
            )

        job_list_obj = self.cmd("az ml job list --max-results 2 -g testrg -w testworkspace")
        job_list_yml = yaml.safe_load(job_list_obj.output)
        assert len(job_list_yml) == 2

        # cli.azure.cli.core.azclierror: (ServiceError) InternalServerError
        all_job_list_obj = self.cmd("az ml job list --all-results -g testrg -w testworkspace")
        all_job_list_yml = yaml.safe_load(all_job_list_obj.output)
        assert len(all_job_list_yml) >= total_jobs

        default_job_list_obj = self.cmd("az ml job list -g testrg -w testworkspace")
        default_job_list_yml = yaml.safe_load(default_job_list_obj.output)
        # If no max-results is provided, we should get at least the jobs that we submitted BUT not more than the default max
        assert len(default_job_list_yml) >= total_jobs
        assert len(default_job_list_yml) <= MAX_LIST_CLI_RESULTS

    def test_job_command_git_path(self) -> None:
        with private_flag():
            job_name_suffix = "-4"
            self.kwargs["command_job_name4"] = "{}{}".format(self.kwargs.get("commandJobName", None), job_name_suffix)
            job_obj = self.cmd(
                "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/command_job/command_job_git_path.yml --name {command_job_name4} -g testrg -w testworkspace"
            )
            job_obj = yaml.safe_load(job_obj.output)
            assert "outputs" in job_obj
            assert "parameters" in job_obj
            assert "properties" in job_obj
            assert "services" in job_obj
            assert "status" in job_obj
            assert "tags" in job_obj
            assert job_obj["type"] == "command"
            assert job_obj["name"] == self.kwargs.get("command_job_name4", None)
            assert job_obj["display_name"] == self.kwargs.get("command_job_name4", None)
            assert job_obj["id"]
            assert "creation_context" in job_obj
            assert "code" in job_obj
            # Delete a key regardless of whether it is in the dictionary for the new name
            self.kwargs.pop("command_job_name4", None)

    # This test is not working. TODO: https://dev.azure.com/msdata/Vienna/_workitems/edit/3372868
    @pytest.mark.skip(reason="Recording and replay not working.")
    def test_job_command_job_create_skip_validation(self) -> None:
        job_name_suffix = "-5"
        self.kwargs["command_job_name3"] = "{}{}".format(self.kwargs.get("commandJobName", None), job_name_suffix)
        create_job_command = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/command_job/command_job_test.yml --name {command_job_name3} -g testrg -w testworkspace"
        from azure.ai.ml.operations import JobOperations

        with patch.object(JobOperations, "_validate") as mock_validate:
            self.cmd(create_job_command + " --skip-validation")
            mock_validate.assert_not_called()
            self.cmd(create_job_command)
            mock_validate.assert_called_once()

    # This test is not working. TODO: https://dev.azure.com/msdata/Vienna/_workitems/edit/3372868
    @pytest.mark.skip(reason="Recording and replay not working.")
    def test_job_command_job_archive_restore(self) -> None:
        job_name_suffix = "-3"
        self.kwargs["command_job_name3"] = "{}{}".format(self.kwargs.get("commandJobName", None), job_name_suffix)
        create_job_command = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/command_job/command_job_test.yml --name {command_job_name3} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)

        # show_job_command = "az ml job show --name {command_job_name3} -g testrg -w testworkspace"
        # job_obj = self.cmd(show_job_command)

        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == self.kwargs.get("command_job_name3", None)

        archive_job_command = "az ml job archive -n {command_job_name3} -g testrg -w testworkspace"
        job_archive_obj = self.cmd(archive_job_command)
        assert job_archive_obj.output == ""

        restore_job_command = "az ml job restore -n {command_job_name3} -g testrg -w testworkspace"
        job_restore_obj = self.cmd(restore_job_command)
        assert job_restore_obj.output == ""
        # Delete a key regardless of whether it is in the dictionary for the new name
        self.kwargs.pop("command_job_name3", None)

    def test_job_environment_version(self) -> None:
        self.cmd(
            "az ml environment create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/environment/environment_colon_version.yml -g testrg -w testworkspace"
        )
        self.cmd("az ml compute create --name testCompute --type AmlCompute --no-wait -g testrg -w testworkspace")
        job_name_suffix = "-4"
        self.kwargs["job_name1"] = "{}{}".format(self.kwargs.get("environmentName", None), job_name_suffix)
        create_job = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/command_job/command_job_environment_version.yml --name {job_name1} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job)
        job_obj = yaml.safe_load(job_obj.output)
        assert "18.04:2022" in job_obj["environment"]
        # Delete a key regardless of whether it is in the dictionary for the new name
        self.kwargs.pop("job_name1", None)

    @pytest.mark.usefixtures("enable_private_preview_features")
    @pytest.mark.skip(reason="TODO: 2161301, Test is failing when executing from recording in ADO")
    def test_job_command_job_with_asset_registry(self) -> None:
        # TODO: Assumes there is a compute called testCompute, which may prevent a successful recording
        with private_flag():
            job_name_suffix = "50"
            # Updating dictionary with the job name
            self.kwargs["command_job_name"] = "{}{}".format(self.kwargs.get("commandJobName", None), job_name_suffix)
            create_job = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/command_job/command_job_registry.yml --name {command_job_name} -g testrg -w testworkspace"
            job_obj = self.cmd(create_job)
            job_obj = yaml.safe_load(job_obj.output)
            assert CURATED_ENV_PREFIX in job_obj["environment"]
            # Delete a key regardless of whether it is in the dictionary for the new name
            self.kwargs.pop("command_job_name1", None)

    @pytest.mark.skip(reason="TODO: 1796036, does not complete when recorded")
    def test_job_sweep_job(self) -> None:
        job_name_suffix = "2"
        self.kwargs["sweep_job_name1"] = "{}{}".format(self.kwargs.get("sweepJobName", None), job_name_suffix)
        create_job_command = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/sweep_job/sweep_job_test.yaml --name {sweep_job_name1} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)

        # show_job_command = "az ml job show --name {sweep_job_name1} -g testrg -w testworkspace"
        # job_obj = self.cmd(show_job_command)

        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == self.kwargs.get("sweep_job_name1", None)
        assert "testCompute" in job_obj["compute"]
        assert job_obj["experiment_name"] == "sdk-cli-v2"
        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/sweep_job/sweep_job_test.yaml", "r"
        ) as f:
            file_obj = yaml.safe_load(f)

        # Check sweep-specific properties. Search space is verified in other testing.
        job_objective = job_obj["objective"]
        for param, value in file_obj["objective"].items():
            assert param in job_objective
            job_value = job_objective.get(param)
            assert value == job_value

        assert file_obj["limits"]["max_total_trials"] == job_obj["limits"]["max_total_trials"]
        assert file_obj["limits"]["max_concurrent_trials"] == job_obj["limits"]["max_concurrent_trials"]
        # assert to_iso_duration_format(file_obj["limits"]["timeout"]) == job_obj["limits"]["timeout"]
        # TODO 1335978 [Joe]: why dose the timeout changed in the response? It should be the same as in the file.

        job_early_termination = job_obj["early_termination"]
        for param, value in file_obj["early_termination"].items():
            assert param in job_early_termination
            job_value = job_early_termination.get(param)
            assert value == job_value
        # Delete a key regardless of whether it is in the dictionary for the new name
        self.kwargs.pop("sweep_job_name1", None)

    @pytest.mark.skip(reason="Can't record - job times out due to max_duration reached")
    def test_job_sweep_job_registry(self) -> None:
        job_name_suffix = "90"
        self.kwargs["sweep_job_name"] = "{}{}".format(self.kwargs.get("sweepJobName", None), job_name_suffix)
        create_job_command = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/sweep_job/sweep_job_test_registry.yaml --name {sweep_job_name} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert REGISTRY_URI_FORMAT in job_obj["trial"]["environment"]

        # Delete a key regardless of whether it is in the dictionary for the new name
        self.kwargs.pop("sweep_job_name", None)

    @pytest.mark.skip(reason="TODO: 1788025, could not re-record this test")
    def test_job_pipeline_job_settings_simple(self) -> None:
        job_name_suffix = "-1"
        self.kwargs["pipeline_job_name1"] = "{}{}".format(self.kwargs.get("pipelineJobName", None), job_name_suffix)
        create_job_command = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/helloworld_pipeline_job_defaults_e2e.yml --name {pipeline_job_name1} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)

        # show_job_command = "az ml job show --name {pipeline_job_name1} -g testrg -w testworkspace"
        # job_obj = self.cmd(show_job_command)

        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == self.kwargs.get("pipeline_job_name1", None)
        assert "default_datastore" in job_obj["settings"]
        assert "cpu-cluster" in job_obj["compute"]

        # sanity check outputs for two of the jobs that have outputs
        hello_job_1 = job_obj["jobs"]["hello_world_component_inline_1"]
        assert hello_job_1["outputs"]
        hello_job_1_outputs = hello_job_1["outputs"]
        assert hello_job_1_outputs["output_path"]

        hello_job_2 = job_obj["jobs"]["hello_world_component_inline_2"]
        assert hello_job_2["outputs"]
        hello_job_2_outputs = hello_job_2["outputs"]
        assert hello_job_2_outputs["output_path"]
        # Delete a key regardless of whether it is in the dictionary for the new name
        self.kwargs.pop("pipeline_job_name1", None)

    @pytest.mark.skip(reason="Unable to get authority configuration for login")
    def test_cli_error(self) -> None:
        create_job_command = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/invalid/with_invalid_component.yml -g testrg -w testworkspace"
        with pytest.raises(ValidationError) as e:
            self.cmd(create_job_command)
        assert "Validation for PipelineJobSchema failed:" in str(e.value)
        # Assert missing environment path
        assert "jobs.hello_world_component.component.environment" in str(e.value)

    @pytest.mark.skip(reason="TODO: 1796036, does not complete when recorded")
    def test_job_pipeline_job_input_paths(self) -> None:
        job_name_suffix = "-3"
        self.kwargs["pipeline_job_name2"] = "{}{}".format(self.kwargs.get("pipelineJobName", None), job_name_suffix)
        create_job_command = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/helloworld_pipeline_job_with_component_output.yml --name {pipeline_job_name2} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)

        # show_job_command = "az ml job show --name {pipeline_job_name2} -g testrg -w testworkspace"
        # job_obj = self.cmd(show_job_command)

        job_obj = yaml.safe_load(job_obj.output)
        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/helloworld_pipeline_job_with_component_output.yml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
        assert job_obj["inputs"]
        assert job_obj["name"] == self.kwargs.get("pipeline_job_name2", None)
        job_obj_inputs = job_obj["inputs"]

        # check all given inputs are in the returned object
        for input_name, input_value in file_obj["inputs"].items():
            assert input_name in job_obj_inputs
            job_input = job_obj_inputs[input_name]
            if isinstance(input_value, dict):
                assert job_input["path"]
                assert isinstance(job_input["path"], str)
                # when submitting a pipeline job, the `mount` mode is changed to `ro_mount` in the service.
                assert input_value["mode"].lower() in job_input["mode"].lower() if input_value["mode"] else True
            else:
                assert job_input == input_value
        # TODO - Currently pipeline outputs are getting dropped by MFE. Uncomment once bug is resolved https://msdata.visualstudio.com/Vienna/_workitems/edit/1356842
        # check all of the given outputs are in the returned object
        # job_obj_outputs = job_obj.get("outputs", None)
        # assert job_obj_outputs
        # for output_name, output_value in file_obj["outputs"].items():
        #     assert output_name in job_obj_outputs
        #     job_output = job_obj_outputs[output_name]
        #     assert output_value["mode"].lower() in job_output["mode"].lower() if output_value["mode"] else True

        # check the default datastore is present
        assert job_obj["settings"]["default_datastore"]

        # assert that all of the jobs are there
        job_obj_jobs = job_obj.get("jobs", None)
        assert job_obj_jobs
        for job_name, job_value in file_obj["jobs"].items():
            submitted_job = job_obj_jobs.get(job_name, None)
            assert submitted_job
            # first check the inputs
            submitted_job_inputs = submitted_job.get("inputs", None)
            assert submitted_job_inputs
            for input_name, input_value in job_value["inputs"].items():
                submitted_job_input = submitted_job_inputs.get(input_name, None)
                assert submitted_job_input

            # next, check the outputs
            submitted_job_outputs = submitted_job.get("outputs", None)
            assert submitted_job_outputs
            for output_name, output_value in job_value["outputs"].items():
                submitted_job_output = submitted_job_outputs.get(output_name, None)
                assert submitted_job_output
                if isinstance(output_value, str):
                    assert output_value == submitted_job_output
                else:
                    assert (
                        output_value["mode"].lower() in submitted_job_output["mode"].lower()
                        if output_value["mode"]
                        else True
                    )
        # Delete a key regardless of whether it is in the dictionary for the new name
        self.kwargs.pop("pipeline_job_name2", None)

    @pytest.mark.skip(reason="Tests always fail.")
    def test_job_pipeline_job_translated_from_command_job(self) -> None:
        job_name_suffix = "-4"
        self.kwargs["pipeline_job_name3"] = "{}{}".format(self.kwargs.get("pipelineJobName", None), job_name_suffix)
        create_job_command = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/helloworld_pipeline_job_defaults_with_command_job_e2e.yml --name {pipeline_job_name3} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)

        # show_job_command = "az ml job show --name {pipeline_job_name3} -g testrg -w testworkspace"
        # job_obj = self.cmd(show_job_command)

        job_obj = yaml.safe_load(job_obj.output)
        print(job_obj)
        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/helloworld_pipeline_job_defaults_with_command_job_e2e.yml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
        assert job_obj["inputs"]
        assert job_obj["name"] == self.kwargs.get("pipeline_job_name3", None)
        job_obj_inputs = job_obj["inputs"]
        # check all given inputs are in the returned object
        for input_name, input_value in file_obj["inputs"].items():
            assert input_name in job_obj_inputs
            job_input = job_obj_inputs[input_name]
            if isinstance(input_value, dict):
                assert job_input["path"]
                assert isinstance(job_input["path"], str)
                assert input_value["mode"].lower() == job_input["mode"].lower() if input_value["mode"] else True
            else:
                assert job_input == input_value

        # assert that all of the jobs are there
        job_obj_jobs = job_obj.get("jobs", None)
        assert job_obj_jobs
        for job_name, job_value in file_obj["jobs"].items():
            submitted_job = job_obj_jobs.get(job_name, None)
            assert submitted_job
            # check the inputs
            submitted_job_inputs = submitted_job.get("inputs", {})
            if len(submitted_job_inputs):
                for input_name, input_value in job_value["inputs"].items():
                    submitted_job_input = submitted_job_inputs.get(input_name, None)
                    assert submitted_job_input
                    # assert submitted_job_input == input_value
        # Delete a key regardless of whether it is in the dictionary for the new name
        self.kwargs.pop("pipeline_job_name3", None)

    @pytest.mark.skip(reason="Recording and replay not working.")
    def test_pipeline_job_validate(self) -> None:
        import json

        error_msg_on_validate = self.cmd(
            "az ml job validate --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/invalid/combo.yml -o json -g testrg -w testworkspace"
        )
        result = json.loads(error_msg_on_validate.output)
        assert pydash.omit(result, "errors.0.location", "warnings.0.location") == {
            "errors": [
                {
                    "message": "Compute not set",
                    "path": "jobs.command_node.compute",
                    "value": None,
                }
            ],
            "warnings": [
                {
                    "message": "Unknown field.",
                    "path": "jobs.command_node.jeff_special_option",
                    "value": {"joo": "bar"},
                }
            ],
            "result": "Failed",
        }

    # This test is not working. TODO: https://dev.azure.com/msdata/Vienna/_workitems/edit/3372868
    @pytest.mark.skip(reason="Recording and replay not working.")
    def test_automl_tabular_classification_job(self) -> None:
        classification_job_name = "{}{}".format(self.kwargs.get("automlClassificationJobName", None), "-1")
        create_job_command = f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_classification_job.yaml --name {classification_job_name} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == classification_job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_classification_job.yaml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(file_obj=file_obj, job_obj=job_obj)

    # This test is not working. TODO: https://dev.azure.com/msdata/Vienna/_workitems/edit/3372868
    @pytest.mark.skip(reason="Recording and replay not working.")
    def test_automl_tabular_forecasting_job(self) -> None:
        forecasting_job_name = "{}{}".format(self.kwargs.get("automlForecastingJobName", None), "-1")
        create_job_command = f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_forecasting_job.yaml --name {forecasting_job_name} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == forecasting_job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_forecasting_job.yaml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(file_obj=file_obj, job_obj=job_obj)

            # Validate forecasting settings
            job_forecasting = job_obj["forecasting"]
            for param, value in file_obj["forecasting"].items():
                assert param in job_forecasting
                job_value = job_forecasting.get(param)
                assert value == job_value

    # This test is not working. TODO: https://dev.azure.com/msdata/Vienna/_workitems/edit/3372868
    @pytest.mark.skip(reason="Recording and replay not working.")
    def test_automl_tabular_regression_job(self) -> None:
        regression_job_name = "{}{}".format(self.kwargs.get("automlRegressionJobName", None), "-1")
        create_job_command = f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_regression_job.yaml --name {regression_job_name} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == regression_job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_regression_job.yaml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(file_obj=file_obj, job_obj=job_obj)

    @pytest.mark.skip(reason="Tests always fail.")
    def test_automl_image_classification_job(self) -> None:
        image_classification_job_name = "{}{}".format(self.kwargs.get("automlImageClassificationJobName", None), "-2")
        create_job_command = f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_classification_job.yaml --name {image_classification_job_name} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == image_classification_job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_classification_job.yaml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(
                file_obj=file_obj, job_obj=job_obj, skip_compute_experiment=True
            )
            self._validate_automl_image_job_properties(file_obj=file_obj, job_obj=job_obj)

    # This test is not working. TODO: https://dev.azure.com/msdata/Vienna/_workitems/edit/3372868
    @pytest.mark.skip(reason="Recording and replay not working.")
    def test_automl_image_classification_automode_job(self) -> None:
        image_classification_automode_job_name = "{}{}".format(
            self.kwargs.get("automlImageClassificationAutoModeJobName", None), "-1"
        )
        create_job_command = f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_classification_automode_job.yaml --name {image_classification_automode_job_name} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == image_classification_automode_job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_classification_automode_job.yaml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(
                file_obj=file_obj, job_obj=job_obj, skip_compute_experiment=True
            )
            self._validate_automl_image_job_properties(file_obj=file_obj, job_obj=job_obj)

    @pytest.mark.skip(reason="Tests always fail.")
    def test_automl_image_classification_multilabel_job(self) -> None:
        image_classification_multilabel_job_name = "{}{}".format(
            self.kwargs.get("automlImageClassificationMultilabelJobName", None), "-2"
        )
        create_job_command = f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_classification_multilabel_job.yaml --name {image_classification_multilabel_job_name} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == image_classification_multilabel_job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_classification_multilabel_job.yaml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(
                file_obj=file_obj, job_obj=job_obj, skip_compute_experiment=True
            )
            self._validate_automl_image_job_properties(file_obj=file_obj, job_obj=job_obj)

    # This test is not working. TODO: https://dev.azure.com/msdata/Vienna/_workitems/edit/3372868
    @pytest.mark.skip(reason="Recording and replay not working.")
    def test_automl_image_classification_multilabel_automode_job(self) -> None:
        image_classification_multilabel_automode_job_name = "{}{}".format(
            self.kwargs.get("automlImageClassificationMultilabelAutoModeJobName", None), "-1"
        )
        create_job_command = f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_classification_multilabel_automode_job.yaml --name {image_classification_multilabel_automode_job_name} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == image_classification_multilabel_automode_job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_classification_multilabel_automode_job.yaml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(
                file_obj=file_obj, job_obj=job_obj, skip_compute_experiment=True
            )
            self._validate_automl_image_job_properties(file_obj=file_obj, job_obj=job_obj)

    @pytest.mark.skip(reason="Tests always fail.")
    def test_automl_image_instance_segmentation_job(self) -> None:
        image_instance_segmentation_job_name = "{}{}".format(
            self.kwargs.get("automlImageInstanceSegmentationJobName", None), "-2"
        )
        create_job_command = f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_instance_segmentation_job.yaml --name {image_instance_segmentation_job_name} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == image_instance_segmentation_job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_instance_segmentation_job.yaml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(
                file_obj=file_obj, job_obj=job_obj, skip_compute_experiment=True
            )
            self._validate_automl_image_job_properties(file_obj=file_obj, job_obj=job_obj)

    # This test is not working. TODO: https://dev.azure.com/msdata/Vienna/_workitems/edit/3372868
    @pytest.mark.skip(reason="Recording and replay not working.")
    def test_automl_image_instance_segmentation_automode_job(self) -> None:
        image_instance_segmentation_automode_job_name = "{}{}".format(
            self.kwargs.get("automlImageInstanceSegmentationAutoModeJobName", None), "-1"
        )
        create_job_command = f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_instance_segmentation_automode_job.yaml --name {image_instance_segmentation_automode_job_name} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == image_instance_segmentation_automode_job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_instance_segmentation_automode_job.yaml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(
                file_obj=file_obj, job_obj=job_obj, skip_compute_experiment=True
            )
            self._validate_automl_image_job_properties(file_obj=file_obj, job_obj=job_obj)

    @pytest.mark.skip(reason="Tests always fail.")
    def test_automl_image_object_detection_job(self) -> None:
        image_object_detection_job_name = "{}{}".format(
            self.kwargs.get("automlImageObjectDetectionJobName", None), "-2"
        )
        create_job_command = f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_object_detection_job.yaml --name {image_object_detection_job_name} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == image_object_detection_job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_object_detection_job.yaml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(
                file_obj=file_obj, job_obj=job_obj, skip_compute_experiment=True
            )
            self._validate_automl_image_job_properties(file_obj=file_obj, job_obj=job_obj)

    # This test is not working. TODO: https://dev.azure.com/msdata/Vienna/_workitems/edit/3372868
    @pytest.mark.skip(reason="Recording and replay not working.")
    def test_automl_image_object_detection_automode_job(self) -> None:
        image_object_detection_automode_job_name = "{}{}".format(
            self.kwargs.get("automlImageObjectDetectionAutoModeJobName", None), "-1"
        )
        create_job_command = f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_object_detection_automode_job.yaml --name {image_object_detection_automode_job_name} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == image_object_detection_automode_job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_image_object_detection_automode_job.yaml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(
                file_obj=file_obj, job_obj=job_obj, skip_compute_experiment=True
            )
            self._validate_automl_image_job_properties(file_obj=file_obj, job_obj=job_obj)

    @pytest.mark.skip(reason="Tests always fail.")
    def test_automl_image_job_limits_outside_sweep(self) -> None:
        image_job_name = "{}{}".format(self.kwargs.get("automlImageObjectDetectionJobName", None), "-1")
        create_job_command = f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/automl_image_job_limits_outside_sweep_mock.yml --name {image_job_name} -g testrg -w testworkspace"
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == image_job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/automl_image_job_limits_outside_sweep_mock.yml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(
                file_obj=file_obj, job_obj=job_obj, skip_compute_experiment=True
            )
            self._validate_automl_image_job_properties(file_obj=file_obj, job_obj=job_obj)

    def _validate_automl_image_job_properties(self, file_obj, job_obj) -> None:
        def validate_internal(file_internal, job_internal) -> None:
            for param, value in file_internal.items():
                assert param in job_internal
                job_value = job_internal.get(param)
                assert value == job_value

        def validate_image_sweep(file_sweep, job_sweep) -> None:
            for input_name, input_value in file_sweep.items():
                assert input_name in job_sweep
                job_input = job_sweep[input_name]
                if isinstance(input_value, dict):
                    for param, value in input_value.items():
                        assert param in job_input
                        job_value = job_input.get(param)
                        assert value == job_value
                else:
                    assert job_input == input_value

        def validate_image_search_space(file_search_space, job_search_space) -> None:
            assert len(file_search_space) == len(job_search_space)
            for l in range(0, len(file_search_space)):
                validate_internal(file_search_space[l], job_search_space[l])

        # Validate sweep settings
        if file_obj.get("sweep"):
            validate_image_sweep(file_obj["sweep"], job_obj["sweep"])
        else:
            assert not job_obj.get("sweep")

        # Validate search_space settings
        if file_obj.get("search_space"):
            validate_image_search_space(file_obj["search_space"], job_obj["search_space"])
        else:
            assert not job_obj.get("search_space")

        # Validate training_parameters settings
        if file_obj.get("training_parameters"):
            validate_internal(file_obj["training_parameters"], job_obj["training_parameters"])
        else:
            assert not job_obj.get("training_parameters")

    def _validate_automl_job_common_properties(
        self, file_obj, job_obj, skip_compute_experiment=False, is_gpu=False
    ) -> None:
        if not skip_compute_experiment:
            if is_gpu:
                assert "gpu-cluster" in job_obj["compute"]
            else:
                assert "cpu-cluster" in job_obj["compute"]
            assert job_obj["experiment_name"] == file_obj["experiment_name"]
        assert job_obj["log_verbosity"] == file_obj["log_verbosity"]
        assert job_obj["primary_metric"] == file_obj["primary_metric"]

        # required as default value is set for some properties in job output
        def validate_internal(file_internal, job_internal) -> None:
            for param, value in file_internal.items():
                assert param in job_internal
                job_value = job_internal.get(param)
                assert value == job_value

        # Validate limits settings
        if file_obj.get("limits"):
            validate_internal(file_obj["limits"], job_obj["limits"])

        # Validate featurization settings
        if file_obj.get("featurization"):
            validate_internal(file_obj["featurization"], job_obj["featurization"])

        # Validate training settings
        if file_obj.get("training"):
            validate_internal(file_obj["training"], job_obj["training"])

    @pytest.mark.skip(reason="Tests always fail.")
    def test_automl_text_classification_job(self) -> None:
        job_name = "{}{}".format(self.kwargs.get("automlTextClassificationJobName", None), "-1")
        create_job_command = (
            f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/ -g testrg -w testworkspace"
            f"automl_text_classification_job.yaml --name {job_name}"
        )
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_text_classification_job.yaml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(file_obj=file_obj, job_obj=job_obj, is_gpu=True)

    @pytest.mark.skip(reason="Tests always fail.")
    def test_automl_text_classification_multilabel_job(self) -> None:
        job_name = "{}{}".format(self.kwargs.get("automlTextMultilabelJobName", None), "-1")
        create_job_command = (
            f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/ -g testrg -w testworkspace"
            f"automl_text_classification_multilabel_job.yaml --name {job_name}"
        )
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_text_classification_multilabel_job.yaml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(file_obj=file_obj, job_obj=job_obj, is_gpu=True)

    @pytest.mark.skip(reason="Tests always fail.")
    def test_automl_text_ner_job(self) -> None:
        job_name = "{}{}".format(self.kwargs.get("automlTextNERJobName", None), "-1")
        create_job_command = (
            f"az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/ -g testrg -w testworkspace"
            f"automl_text_ner_job.yaml --name {job_name}"
        )
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == job_name

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/automl_job/e2e_configs/automl_text_ner_job.yaml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            self._validate_automl_job_common_properties(file_obj=file_obj, job_obj=job_obj, is_gpu=True)

    """
    @pytest.mark.public_preview_only
    def test_job_automl_job_public_preview_sanity(self) -> None:
        with pytest.raises(CLIError) as ex:
            self.cmd("az ml job create -n automl-sanity-test -f ./src/machinelearningservices/azext_mlv2/tests/test_configs/jobs/automl_job_mock.yaml -g testrg -w testworkspace")
        assert "Unsupported job type: automl_job" in str(ex)
    """

    @pytest.mark.skip(reason="TODO: 1788034, could not re-record this test")
    def test_component_job_with_registry_uri(self):
        component_obj = self.cmd(
            "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/component_from_registry.yml -n {componentName} -g testrg -w testworkspace"
        )
        component_create_obj = yaml.safe_load(component_obj.output)
        assert (
            component_create_obj["jobs"]["component_1"]["component"]
            == "azureml://registries/testFeed/components/my_hello_world_asset_2/versions/1"
        )

    @pytest.mark.skip(reason="TODO: 1796036, does not complete when recorded")
    def test_pipeline_job_with_automl_node(self):
        job_name_suffix = "-11"
        self.kwargs["pipeline_job_name11"] = "{}{}".format(self.kwargs.get("pipelineJobName", None), job_name_suffix)
        create_job_command = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/jobs_with_automl_nodes/onejob_automl_regression.yml --name {pipeline_job_name11} -g testrg -w testworkspace"
        # check if pipeline with automl node can submit
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == self.kwargs["pipeline_job_name11"]

        with open(
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/jobs_with_automl_nodes/onejob_automl_regression.yml",
            "r",
        ) as f:
            file_obj = yaml.safe_load(f)
            file_obj = file_obj["jobs"]["hello_automl_regression"]
            job_obj = job_obj["jobs"]["hello_automl_regression"]

            self._validate_automl_job_common_properties(
                file_obj=file_obj, job_obj=job_obj, skip_compute_experiment=True
            )

    @pytest.mark.skip(reason="TODO: 1796036, does not complete when recorded")
    def test_pipeline_job_with_parallel_node(self):
        job_name_suffix = "-12"
        self.kwargs["pipeline_job_name12"] = "{}{}".format(self.kwargs.get("pipelineJobName", None), job_name_suffix)
        create_job_command = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/helloworld_pipeline_job_defaults_with_parallel_job_file_dataset_input_e2e.yml --name {pipeline_job_name12} -g testrg -w testworkspace"
        # check if pipeline with parallel node can submit
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == self.kwargs["pipeline_job_name12"]

    @pytest.mark.skip(reason="TODO: 2161301, Test is failing when executing from recording in ADO")
    def test_pipeline_job_with_registry_env(self):
        job_name_suffix = "-14"
        self.kwargs["pipeline_job_name14"] = "{}{}".format(self.kwargs.get("pipelineJobName", None), job_name_suffix)
        create_job_command = "az ml job create -f ./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/hello-pipeline-abc.yml --name {pipeline_job_name14} --set jobs.a.environment=azureml://registries/azureml-dev/environments/sklearn-10-ubuntu2004-py38-cpu/versions/19.dev6 -g testrg -w testworkspace"
        # check if pipeline with parallel node can submit
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == self.kwargs["pipeline_job_name14"]

    @pytest.mark.skip(reason="Tests always fail.")
    def test_pipeline_job_with_pipeline_node(self):
        job_name_suffix = "-15"
        self.kwargs["pipeline_job_name15"] = "{}{}".format(self.kwargs.get("pipelineJobName", None), job_name_suffix)
        create_job_command = "az ml job create -f ./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/pipeline_job_with_pipeline_component.yml --name {pipeline_job_name15} -g testrg -w testworkspace"
        # check if pipeline with pipeline node can submit
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == self.kwargs["pipeline_job_name15"]

    @pytest.mark.skip(reason="Tests always fail.")
    def test_pipeline_job_with_parameter_group(self):
        job_name_suffix = "-16"
        self.kwargs["pipeline_job_name16"] = "{}{}".format(self.kwargs.get("pipelineJobName", None), job_name_suffix)
        create_job_command = "az ml job create -f ./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/pipeline_job_with_parameter_group.yml --name {pipeline_job_name16} -g testrg -w testworkspace"
        # check if pipeline with parallel node can submit
        job_obj = self.cmd(create_job_command)
        job_obj = yaml.safe_load(job_obj.output)
        assert job_obj["name"] == self.kwargs["pipeline_job_name16"]

    @pytest.mark.skip(reason="TODO: 2174601 - Test is failing with RG issues when executing in ADO")
    def test_custom_property_logging(self) -> None:
        job_name_suffix = "-17"
        self.kwargs["command_job_name5"] = "{}{}".format(self.kwargs.get("commandJobName", None), job_name_suffix)
        create_job_command = "az ml job create --file ./src/machinelearningservices/azext_mlv2/tests/test_configs/command_job/command_job_test.yml --name {command_job_name5} --set compute=diondra -g testrg -w testworkspace"
        with patch("azure.cli.core.azclierror.telemetry.add_extension_event") as mock_add_event:
            self.cmd(create_job_command)
            mock_custom_properties_error = {"ml.cli.event": {"error_code": "UserError", "raw_msg": ""}}
            mock_add_event.assert_called_once_with(mock_custom_properties_error)


@pytest.mark.private_preview_only
@pytest.mark.usefixtures("disable_private_preview_features")
class JobPrivatePreviewScenarioTest(MLBaseScenarioTest):
    @pytest.mark.skip(reason="TODO: 1796036, does not complete when recorded")
    def test_pipeline_job_private_preview_features(self):
        test_paths = [
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/jobs_with_automl_nodes/onejob_automl_regression.yml",
            "./src/machinelearningservices/azext_mlv2/tests/test_configs/pipeline_jobs/helloworld_pipeline_job_defaults_with_parallel_job_file_component_input_e2e.yml",
        ]
        for test_path in test_paths:
            create_job_command = "az ml job create --file {} -g testrg -w testworkspace".format(test_path)
            error_msg = (
                "is a private preview feature, "
                "please set environment variable AZURE_ML_CLI_PRIVATE_FEATURES_ENABLED to true to use it."
            )
            with pytest.raises(UserErrorException) as e:
                self.cmd(create_job_command)
            assert error_msg in str(e.value)


