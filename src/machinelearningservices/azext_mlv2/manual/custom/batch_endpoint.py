# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for
# license information.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------
import logging
import time

from azure.ai.ml._utils._arm_id_utils import parse_name_version, remove_aml_prefix
from azure.ai.ml._utils._endpoint_utils import polling_wait
from azure.ai.ml.constants._common import ARM_ID_PREFIX, AssetTypes, InputTypes
from azure.ai.ml.constants._endpoint import BatchEndpointInvoke, EndpointYamlFields
from azure.ai.ml.entities import BatchEndpoint, Endpoint
from azure.ai.ml.entities._inputs_outputs import Input, Output
from azure.ai.ml.entities._load_functions import _try_load_yaml_dict, load_batch_endpoint
from azure.cli.core.commands import LongRunningOperation
from azure.core.exceptions import ResourceNotFoundError

from .raise_error import log_and_raise_error
from .utils import (
    _dump_entity_with_warnings,
    convert_str_to_dict,
    get_ml_client,
    merged_nested_dictionaries,
    validate_and_split_output_path,
)

module_logger = logging.getLogger(__name__)
module_logger.propagate = 0


def ml_batch_endpoint_show(cmd, resource_group_name, workspace_name, name):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        endpoint = ml_client.batch_endpoints.get(name=name)
        return endpoint.dump()
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def _ml_batch_endpoint_show(cmd, resource_group_name, workspace_name, name=None, file=None):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        if not name:
            endpoint = load_batch_endpoint(source=file)
            name = endpoint.name
        if file:
            return load_batch_endpoint(source=file, params_override=[{"name": name.lower()}])
        return ml_client.batch_endpoints.get(name=name)
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_batch_endpoint_create(
    cmd, resource_group_name, workspace_name, file=None, name=None, no_wait=False, params_override=None, **kwargs  # pylint: disable=unused-argument
):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )
    params_override = params_override or []

    try:
        if name:
            # MFE is case-insensitive for Name. So convert the name into lower case here.
            params_override.append({"name": name.lower()})
        endpoint = load_batch_endpoint(source=file, params_override=params_override)
        endpoint_poller = ml_client.begin_create_or_update(endpoint)

        if no_wait:
            module_logger.warning(
                "Batch endpoint update/create request initiated. "
                "Status can be checked using `az ml batch-endpoint show -n %s`\n", endpoint.name
            )
        else:
            endpoint = LongRunningOperation(cmd.cli_ctx)(endpoint_poller)
        if isinstance(endpoint, BatchEndpoint):
            return _dump_entity_with_warnings(endpoint)
    except Exception as err:  # pylint: disable=broad-exception-caught
        yaml_operation = bool(file)
        log_and_raise_error(err, debug, yaml_operation=yaml_operation)


def ml_batch_endpoint_delete(
    cmd,
    resource_group_name,
    workspace_name,
    name,
    no_wait=False,
):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        start_time = time.time()
        result = ml_client.batch_endpoints.begin_delete(name=name)
        if no_wait:
            module_logger.warning(
                "Delete request initiated. Status can be checked using `az ml batch-endpoint show -n %s`\n", name
            )
        else:
            msg = "Deleting batch endpoint %s " % name
            return polling_wait(result, msg, start_time)
        return result
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_batch_endpoint_list(cmd, resource_group_name, workspace_name):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        results = ml_client.batch_endpoints.list()
        return [_dump_entity_with_warnings(x) for x in results]
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_batch_endpoint_update(
    cmd,
    resource_group_name,
    workspace_name,
    defaults=None,
    name=None,
    file=None,  # pylint: disable=unused-argument
    no_wait=False,
    parameters=None,
    **kwargs,  # pylint: disable=unused-argument
) -> None:
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        if name:
            parameters.name = name

        if defaults and isinstance(defaults, str):
            defaults = convert_str_to_dict(defaults)
            parameters.defaults = defaults

        endpoint_return = ml_client.begin_create_or_update(entity=parameters)

        if no_wait:
            module_logger.warning(
                "Batch endpoint update/create request initiated. "
                "Status can be checked using `az ml batch-endpoint show -n %s`\n", parameters.name
            )
        else:
            endpoint_return = LongRunningOperation(cmd.cli_ctx)(endpoint_return)

        if isinstance(endpoint_return, Endpoint):
            return _dump_entity_with_warnings(endpoint_return)
        return endpoint_return

    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_batch_endpoint_invoke(  # pylint: disable=too-many-locals,too-many-branches
    cmd,
    resource_group_name,
    workspace_name,
    name,
    input=None,  # pylint: disable=redefined-builtin
    batch_deployment_name=None,
    input_type=None,  # uri_file, uri_folder
    output_path=None,  # --output-path azureml://datastores/<datastore-name>/<path-on-datastore>
    job_name=None,
    mini_batch_size=None,
    instance_count=None,
    params_override=None,
    file=None,
    inputs=None,
    outputs=None,
    experiment_name=None,
) -> str:
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )
    params_override = params_override or []

    try:  # pylint: disable=too-many-nested-blocks
        if batch_deployment_name:
            ml_client.batch_endpoints._validate_deployment_name(  # pylint: disable=protected-access
                name, batch_deployment_name
            )

        if file:
            batch_endpoint_invocation = _try_load_yaml_dict(file)
            inputs_dict = batch_endpoint_invocation.get(BatchEndpointInvoke.INPUTS)
            if inputs_dict:
                inputs = _parse_inputs(inputs_dict)
            outputs_dict = batch_endpoint_invocation.get(BatchEndpointInvoke.OUTPUTS)
            if outputs_dict:
                outputs = {}
                for key, data in outputs_dict.items():
                    output = Output(
                        type=data.get(BatchEndpointInvoke.TYPE),
                        path=data.get(BatchEndpointInvoke.PATH),
                        mode=data.get(BatchEndpointInvoke.MODE),
                    )
                    outputs[key] = output
            if not batch_deployment_name:
                batch_deployment_name = batch_endpoint_invocation.get(BatchEndpointInvoke.DEPLOYMENT)
            if not name:
                name = batch_endpoint_invocation.get(BatchEndpointInvoke.ENDPOINT)
        elif input or input_type:
            if input and not input_type:
                if input.startswith(ARM_ID_PREFIX):  # azureml:data:version
                    input = remove_aml_prefix(input)
                    asset_name, asset_version = parse_name_version(input)
                    try:
                        data = ml_client.data.get(asset_name, asset_version)
                        # invoke input should be of type Input based on the data type
                        if data.type == AssetTypes.URI_FOLDER:
                            input = Input(type=AssetTypes.URI_FOLDER, path=data.id)
                        elif data.type == AssetTypes.URI_FILE:
                            input = Input(type=AssetTypes.URI_FILE, path=data.id)
                    except ResourceNotFoundError:
                        # We have a case that the input is not a registered data asset
                        msg = (
                            "Value passed to --input is not a supported data. "
                            "Please use an Azure Machine Learning registered V2 data asset "
                            "(with the type of either `uri_file` or `uri_folder`). Refer to how-to guide for "
                            "batch endpoint for more information on supported input data types."
                        )
                        raise ValueError(msg)
                else:  # ./path
                    input = Input(type=AssetTypes.URI_FOLDER, path=input)
            elif input_type == AssetTypes.URI_FOLDER:  # --input-type uri_folder
                input = Input(type=AssetTypes.URI_FOLDER, path=input)
            elif input_type == AssetTypes.URI_FILE:  # --input-type uri_file
                input = Input(type=AssetTypes.URI_FILE, path=input)
            else:
                raise ValueError(
                    "Unsupported input please use either a path on the datastore, public URI, "
                    "a registered data asset, or a local folder path."
                )

        if params_override:
            params_override = merged_nested_dictionaries(params_override)
            remove_params = []
            for params in params_override:
                remove_params.append(params)
                inputs_override = params.get("inputs", None)
                if inputs_override:
                    if isinstance(inputs_override, dict):
                        inputs = _parse_inputs(inputs_override)
                    else:
                        raise ValueError(
                            "Unsupported 'inputs' type, it should be a dictionary. Example: inputs.keyName=value"
                        )
                outputs_override = params.get("outputs", None)
                if outputs_override:
                    outputs = {}
                    for key, data in outputs_override.items():
                        output = Output(
                            type=data.get(BatchEndpointInvoke.TYPE),
                            path=data.get(BatchEndpointInvoke.PATH),
                            mode=data.get(BatchEndpointInvoke.MODE),
                        )
                        outputs[key] = output
            # needed to remove input and output params from params_override
            for params in remove_params:
                params_override.remove(params)
        properties = {}

        if mini_batch_size:
            params_override.append({EndpointYamlFields.MINI_BATCH_SIZE: mini_batch_size})
        if instance_count:
            params_override.append({EndpointYamlFields.BATCH_JOB_INSTANCE_COUNT: instance_count})
        if output_path:
            output = validate_and_split_output_path(output_path)  # This is until the API is ready
            params_override.append({EndpointYamlFields.BATCH_JOB_OUTPUT_PATH: output.path})
            params_override.append({EndpointYamlFields.BATCH_JOB_OUTPUT_DATSTORE: "azureml:" + output.datastore})
        if job_name:
            params_override.append({EndpointYamlFields.BATCH_JOB_NAME: job_name})
        if experiment_name:
            properties["experimentName"] = experiment_name

        if properties:
            params_override.append({EndpointYamlFields.BATCH_JOB_PROPERTIES: properties})

        kwargs = {
            "logging_enable": debug,
        }

        return ml_client.batch_endpoints.invoke(
            endpoint_name=name,
            input=input,
            inputs=inputs,
            outputs=outputs,
            deployment_name=batch_deployment_name,
            params_override=params_override,
            **kwargs,
        )
    except Exception as err:  # pylint: disable=broad-exception-caught
        # Bug for service side fix: https://msdata.visualstudio.com/Vienna/_workitems/edit/1413104
        # Once service throws correct exception we will remove this exception handling.
        if isinstance(err, ResourceNotFoundError) and batch_deployment_name is None:
            log_and_raise_error("Set a default deployment or provide a deployment name through cli command.")
        else:
            log_and_raise_error(err, debug)


def _parse_inputs(inputs_dict):
    inputs = {}
    if inputs_dict:
        for key, data in inputs_dict.items():
            if isinstance(data, str):
                inputs[key] = Input(type=InputTypes.STRING, default=data)
            elif isinstance(data, bool):
                inputs[key] = Input(type=InputTypes.BOOLEAN, default=data)
            elif isinstance(data, int):
                inputs[key] = Input(type=InputTypes.INTEGER, default=data)
            elif isinstance(data, float):
                inputs[key] = Input(type=InputTypes.NUMBER, default=data)
            else:

                if data.get(BatchEndpointInvoke.TYPE) in [InputTypes.NUMBER, InputTypes.INTEGER]:
                    inputs[key] = Input(
                        type=data.get(BatchEndpointInvoke.TYPE, None),
                        default=data.get(BatchEndpointInvoke.DEFAULT, None),
                        min=data.get(BatchEndpointInvoke.MIN, None),
                        max=data.get(BatchEndpointInvoke.MAX, None),
                    )
                elif data.get(BatchEndpointInvoke.TYPE) in [InputTypes.STRING, InputTypes.BOOLEAN]:
                    inputs[key] = Input(
                        type=data.get(BatchEndpointInvoke.TYPE, None),
                        default=data.get(BatchEndpointInvoke.DEFAULT, None),
                    )
                else:
                    inputs[key] = Input(
                        type=data.get(BatchEndpointInvoke.TYPE, None),
                        path=data.get(BatchEndpointInvoke.PATH, None),
                        mode=data.get(BatchEndpointInvoke.MODE, None),
                    )
    return inputs


def ml_batch_endpoint_list_jobs(cmd, resource_group_name, workspace_name, name):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        return ml_client.batch_endpoints.list_jobs(endpoint_name=name)
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)
