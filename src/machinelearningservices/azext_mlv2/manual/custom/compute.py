# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for
# license information.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------
from itertools import islice
import os
import subprocess
from typing import Dict, List

from knack.log import get_logger

from azure.ai.ml.constants._common import COMPUTE_UPDATE_ERROR
from azure.ai.ml.constants._compute import ComputeType
from azure.ai.ml.entities._load_functions import load_compute
from azure.cli.core.commands import LongRunningOperation
from azure.ai.ml.entities import ServiceInstance
from azure.ai.ml.exceptions import (
    ErrorCategory,
    ErrorTarget,
    ValidationErrorType,
    ValidationException,
)

from .raise_error import log_and_raise_error
from .utils import _dump_entity_with_warnings, get_ml_client
from ._ssh_command import get_ssh_command, has_ssh_dependencies_installed, ssh_connector_file_path_space_message

module_logger = get_logger(__name__)

IDENTITY_ERROR = "Identity_type can only be either of 'SystemAssigned', 'UserAssigned'"


def ml_compute_list(cmd, resource_group_name, workspace_name, type=None, max_results=None):  # pylint: disable=redefined-builtin
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        if max_results:
            results = islice(ml_client.compute.list(compute_type=type), int(max_results))
        else:
            results = ml_client.compute.list(compute_type=type)
        return [_dump_entity_with_warnings(x) for x in results]
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_compute_show(cmd, resource_group_name, workspace_name, name):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        return _dump_entity_with_warnings(ml_client.compute.get(name=name))
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_compute_list_nodes(cmd, resource_group_name, workspace_name, name):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        nodes = ml_client.compute.list_nodes(name=name)
        return [_dump_entity_with_warnings(x) for x in nodes]
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_compute_create(
    cmd,
    resource_group_name,
    workspace_name,
    name=None,
    type=None,  # pylint: disable=redefined-builtin
    vnet_name=None,
    subnet=None,
    admin_username=None,
    admin_password=None,
    ssh_key_value=None,
    ssh_public_access_enabled=None,
    file=None,
    size=None,
    no_wait=False,
    user_tenant_id=None,
    user_object_id=None,
    min_instances=None,
    max_instances=None,
    idle_time_before_scale_down=None,
    description=None,
    identity_type=None,
    user_assigned_identities=None,
    enable_node_public_ip=None,
    tier=None,
    tags=None,
    location=None,
    params_override=None,
):  # pylint: disable=too-many-locals,too-many-branches
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )
    params_override = params_override or []

    if name:
        params_override.append({"name": name})
    if type:
        params_override.append({"type": type.lower()})
    if vnet_name:
        params_override.append({"network_settings.vnet_name": vnet_name})
    if subnet:
        params_override.append({"network_settings.subnet": subnet})
    if admin_username:
        params_override.append({"ssh_settings.admin_username": admin_username})
    if admin_password:
        params_override.append({"ssh_settings.admin_password": admin_password})
    if ssh_key_value:
        params_override.append({"ssh_settings.ssh_key_value": ssh_key_value})
    if size:
        params_override.append({"size": size})
    if user_tenant_id:
        params_override.append({"create_on_behalf_of.user_tenant_id": user_tenant_id})
    if user_object_id:
        params_override.append({"create_on_behalf_of.user_object_id": user_object_id})
    if min_instances:
        params_override.append({"min_instances": min_instances})
    if max_instances:
        params_override.append({"max_instances": max_instances})
    if idle_time_before_scale_down:
        params_override.append({"idle_time_before_scale_down": idle_time_before_scale_down})
    if description:
        params_override.append({"description": description})
    if identity_type:
        params_override.append({"identity.type": identity_type})
    if user_assigned_identities:
        identities = _process_user_assigned_identities(user_assigned_identities)
        params_override.append({"identity.user_assigned_identities": identities})
    if enable_node_public_ip:
        params_override.append({"enable_node_public_ip": enable_node_public_ip})
    if tier:
        params_override.append({"tier": tier})
    if ssh_public_access_enabled:
        params_override.append({"ssh_public_access_enabled": ssh_public_access_enabled})
    if tags:
        params_override.append({"tags": tags})
    if location:
        params_override.append({"location": location})

    try:
        compute = load_compute(source=file, params_override=params_override)
        if compute.type == "synapsespark":
            raise ValueError(
                "Create operation not supported for synapsespark compute type. "
                "Please try attach operation for attaching synapsespark compute"
            )
        compute = ml_client.begin_create_or_update(compute)
        if not no_wait:
            compute = LongRunningOperation(cmd.cli_ctx)(compute)
        return _dump_entity_with_warnings(compute)
    except Exception as e:  # pylint: disable=broad-exception-caught
        log_and_raise_error(e, debug)


def ml_compute_delete(cmd, resource_group_name, workspace_name, name, no_wait=False):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        compute = ml_client.compute.begin_delete(name=name)
        if not no_wait:
            compute = LongRunningOperation(cmd.cli_ctx)(compute)
        return compute
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_compute_start(cmd, resource_group_name, workspace_name, name, no_wait=False):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        compute = ml_client.compute.begin_start(name=name)
        if not no_wait:
            compute = LongRunningOperation(cmd.cli_ctx)(compute)
        return compute
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_compute_stop(cmd, resource_group_name, workspace_name, name, no_wait=False):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        compute = ml_client.compute.begin_stop(name=name)
        if not no_wait:
            compute = LongRunningOperation(cmd.cli_ctx)(compute)
        return compute
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_compute_restart(cmd, resource_group_name, workspace_name, name, no_wait=False):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        compute = ml_client.compute.begin_restart(name=name)
        if not no_wait:
            compute = LongRunningOperation(cmd.cli_ctx)(compute)
        return compute
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_compute_list_sizes(cmd, resource_group_name, workspace_name, location=None, type=None):  # pylint: disable=redefined-builtin
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        list_sizes = ml_client.compute.list_sizes(location=location, compute_type=type)
        return [_dump_entity_with_warnings(x) for x in list_sizes]
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_compute_list_usage(cmd, resource_group_name, workspace_name, location=None):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        usagelist = ml_client.compute.list_usage(location=location)
        result = []
        for x in usagelist:
            result.append(_dump_entity_with_warnings(x))
        return result
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_compute_connect_ssh(cmd, resource_group_name, workspace_name, name, private_key_file_path=None):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        if private_key_file_path and not os.path.isfile(private_key_file_path):
            raise ValidationException(
                message="There is no file on target path: {}".format(private_key_file_path),
                target=ErrorTarget.JOB,
                no_personal_data_message="There is no file on target path",
                error_category=ErrorCategory.USER_ERROR,
                error_type=ValidationErrorType.FILE_OR_FOLDER_NOT_FOUND,
            )
        compute = ml_client.compute.get(name=name)
        if compute.type != ComputeType.COMPUTEINSTANCE:
            log_and_raise_error("connect-ssh is only for compute instance")
        if not has_ssh_dependencies_installed():
            return

        print("passed websockets package check")

        # create proxy endpoint for CI based on endpoint for jupyter
        # TODO: Improve with a call to get proxyendpoint from CI, requires API
        jupyter = [f["endpoint_uri"] for f in compute.services if f["display_name"] == "Jupyter"][0]
        proxyEndpoint = jupyter.replace(name, f"{name}-22").replace("https://", "wss://").replace("/tree/", "")

        services_dict = {
            "ssh": ServiceInstance(type="SSH", status="Running", properties={"ProxyEndpoint": proxyEndpoint})
        }
        path_has_space, ssh_command = get_ssh_command(services_dict, 0, private_key_file_path)
        print(f"ssh_command: {ssh_command}")
        if path_has_space:
            module_logger.error(ssh_connector_file_path_space_message())
        else:
            subprocess.call(ssh_command, shell=True)
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def _ml_compute_update(
    cmd,
    resource_group_name,
    workspace_name,
    name,
    max_instances=None,
    min_instances=None,
    idle_time_before_scale_down=None,
    identity_type=None,
    user_assigned_identities=None,  # pylint: disable=unused-argument
    tags=None,
    parameters: Dict = None,
    no_wait: bool = False,
):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    compute_type = parameters["type"]
    if compute_type not in (
        ComputeType.AMLCOMPUTE,
        ComputeType.KUBERNETES,
        ComputeType.COMPUTEINSTANCE,
        ComputeType.SYNAPSESPARK,
    ):
        log_and_raise_error(COMPUTE_UPDATE_ERROR.format(name, compute_type))

    params_override = [{k: v} for k, v in parameters.items() if k != "ssh_settings"]

    if name:
        params_override.append({"name": name})
    if max_instances:
        params_override.append({"max_instances": max_instances})
    if min_instances:
        params_override.append({"min_instances": min_instances})
    if idle_time_before_scale_down:
        params_override.append({"idle_time_before_scale_down": idle_time_before_scale_down})
    if identity_type:
        params_override.append({"identity.type": identity_type})
        # This is a bug that is being fixed
        # This is needed as AMLCompute/CI only allows either UAI for SAI
        if identity_type in ["SystemAssigned", "system_assigned"]:
            params_override.append({"identity": {"type": "system_assigned"}})
    if user_assigned_identities:
        identities = _process_user_assigned_identities(user_assigned_identities)
        params_override.append({"identity.user_assigned_identities": identities})
    if tags:
        params_override.append({"tags": tags})

    try:
        compute = ml_client.compute.begin_update(load_compute(None, params_override=params_override))
        if not no_wait:
            compute = LongRunningOperation(cmd.cli_ctx)(compute)
        return _dump_entity_with_warnings(compute)
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def _validate_identity(identity_type, user_assigned_identities):  # pylint: disable=unused-argument
    if identity_type:
        if identity_type not in ["SystemAssigned", "UserAssigned", "None"]:
            raise ValueError(IDENTITY_ERROR)


# Convert comma separated strings to a list of dictionaries
def _process_user_assigned_identities(resource_ids: str) -> List[dict]:
    return [{"resource_id": resource_id} for resource_id in resource_ids.split(",")]


def ml_compute_attach(
    cmd,
    resource_group_name,
    workspace_name,
    name=None,
    type=None,  # pylint: disable=redefined-builtin
    resource_id=None,
    admin_username=None,
    admin_password=None,
    ssh_port=None,
    no_wait=False,
    ssh_private_key_file=None,
    file=None,
    namespace=None,
    identity_type=None,
    user_assigned_identities=None,
):
    _validate_identity(identity_type, user_assigned_identities)
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    params_override = []
    if name:
        params_override.append({"name": name})
    if ssh_private_key_file:
        params_override.append({"ssh_settings.ssh_private_key_file": ssh_private_key_file})
    if admin_username:
        params_override.append({"ssh_settings.admin_username": admin_username})
    if admin_password:
        params_override.append({"ssh_settings.admin_password": admin_password})
    if ssh_port:
        params_override.append({"ssh_settings.ssh_port": ssh_port})
    if type:
        params_override.append({"type": type})
    if resource_id:
        params_override.append({"resource_id": resource_id})
    if namespace:
        params_override.append({"namespace": namespace})
    if identity_type:
        params_override.append({"identity.type": identity_type})
    if user_assigned_identities:
        identities = _process_user_assigned_identities(user_assigned_identities)
        params_override.append({"identity.user_assigned_identities": identities})
    try:
        compute = load_compute(source=file, params_override=params_override)
        if compute.type == "kubernetes" or compute.type == "synapsespark":
            if not compute.resource_id:
                raise ValueError(
                    'The "resource_id" is a required parameter for attaching a kubernetes or synapse compute!'
                )
        compute = ml_client.begin_create_or_update(compute)
        if not no_wait:
            compute = LongRunningOperation(cmd.cli_ctx)(compute)
        return _dump_entity_with_warnings(compute)
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_compute_detach(cmd, resource_group_name, workspace_name, name, no_wait=False):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )

    try:
        compute = ml_client.compute.begin_delete(
            name=name,
            action="Detach",
        )
        if not no_wait:
            compute = LongRunningOperation(cmd.cli_ctx)(compute)
        return _dump_entity_with_warnings(compute)

    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_compute_enable_sso(
    cmd,
    resource_group_name,
    workspace_name,
    name,
    disable: bool = False
):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name, workspace_name=workspace_name
    )
    try:
        if disable:
            ml_client.compute.enable_sso(name=name, enable_sso=False)
        else:
            ml_client.compute.enable_sso(name=name)
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)
