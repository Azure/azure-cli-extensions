# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for
# license information.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------
from itertools import islice

from azure.ai.ml.entities._assets import Model
from azure.ai.ml.entities._load_functions import load_model, load_model_package
from azure.ai.ml.exceptions import ErrorCategory, ErrorTarget, ValidationErrorType, ValidationException

from .raise_error import log_and_raise_error
from .utils import (_dump_entity_with_warnings, get_list_view_type, get_ml_client,
                    telemetry_log_info)
from ._telemetry.structure import ModelInfo
from ._telemetry._util import _set_path_type


def ml_model_create(
    cmd,
    resource_group_name=None,
    workspace_name=None,
    name=None,
    version=None,
    file=None,
    path=None,
    type=None,  # pylint: disable=redefined-builtin
    description=None,
    tags=None,
    stage=None,
    registry_name=None,
    params_override=None,
    datastore=None,
    no_wait=None,  # pylint: disable=unused-argument
):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx,
        resource_group_name=resource_group_name,
        workspace_name=workspace_name,
        registry_name=registry_name,
    )
    params_override = params_override or []

    if name:
        params_override.append({"name": name})
    if version:
        params_override.append({"version": version})
    if path:
        params_override.append({"path": path})
    if type:
        params_override.append({"type": type})
    if description:
        params_override.append({"description": description})
    if tags:
        params_override.append({"tags": tags})
    if stage:
        params_override.append({"stage": stage})
    if datastore:
        params_override.append({"datastore": datastore})

    model_info = ModelInfo()
    try:
        if file:
            model = load_model(source=file, params_override=params_override)
            if model and model.type:
                model_info.model_type = model.type
            if model and model.path:
                model_info.path_type = _set_path_type(model.path)
        elif path and name:
            # no yaml, cli param specification
            model = Model(
                name=name,
                version=version,
                path=path,
                description=description,
                tags=tags,
                stage=stage,
                type=type,
                datastore=datastore,
            )
            if model and model.type:
                model_info.model_type = model.type
            if model and model.path:
                model_info.path_type = _set_path_type(model.path)
        else:
            msg = "Please provide either --file/-f  or --name/-n and --path/-p."
            raise ValidationException(
                message=msg,
                no_personal_data_message=msg,
                target=ErrorTarget.MODEL,
                error_category=ErrorCategory.USER_ERROR,
                error_type=ValidationErrorType.MISSING_FIELD,
            )
        model = ml_client.create_or_update(model)
        return _dump_entity_with_warnings(model)
    except Exception as err:  # pylint: disable=broad-exception-caught
        yaml_operation = bool(file)
        log_and_raise_error(err, debug, yaml_operation=yaml_operation)
    finally:
        telemetry_log_info(model_info.__dict__)


def ml_model_download(
    cmd, name, version, workspace_name=None, resource_group_name=None, download_path=None, registry_name=None
):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx,
        resource_group_name=resource_group_name,
        workspace_name=workspace_name,
        registry_name=registry_name,
    )

    try:
        if not download_path:
            download_path = cmd.cli_ctx.local_context.current_dir
        ml_client.models.download(name=name, version=version, download_path=download_path)
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_model_list(
    cmd,
    resource_group_name=None,
    workspace_name=None,
    name=None,
    stage=None,
    max_results=None,
    include_archived=False,
    archived_only=False,
    registry_name=None,
):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx,
        resource_group_name=resource_group_name,
        workspace_name=workspace_name,
        registry_name=registry_name,
    )

    try:
        list_view_type = get_list_view_type(include_archived=include_archived, archived_only=archived_only)
        if max_results:
            results = islice(
                ml_client.models.list(name=name, stage=stage, list_view_type=list_view_type),
                int(max_results),
            )
        else:
            results = ml_client.models.list(name=name, stage=stage, list_view_type=list_view_type)
        return [_dump_entity_with_warnings(x) for x in results]
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def _ml_model_update(cmd, resource_group_name, parameters, workspace_name=None, registry_name=None, stage=None):
    params_override = []
    if stage:
        params_override.append({"stage": stage})
    # The State of Assets specifies the only difference for PrP in update is that update cannot create a new model.
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name,
        workspace_name=workspace_name, registry_name=registry_name
    )

    try:
        # Set unknown to EXCLUDE so that marshmallow doesn't raise on dump only fields.
        model = Model._load(data=parameters, params_override=params_override)  # pylint: disable=protected-access
        updated_model = ml_client.create_or_update(model)
        return _dump_entity_with_warnings(updated_model)
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_model_archive(cmd, name, resource_group_name=None, workspace_name=None,
                     registry_name=None, version=None, label=None):
    ml_client, _ = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name,
        workspace_name=workspace_name, registry_name=registry_name
    )

    return ml_client.models.archive(name=name, version=version, label=label)


def ml_model_restore(cmd, name, resource_group_name=None, workspace_name=None,
                     registry_name=None, version=None, label=None):
    ml_client, _ = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name,
        workspace_name=workspace_name, registry_name=registry_name
    )

    return ml_client.models.restore(name=name, version=version, label=label)


def _ml_model_show(cmd, resource_group_name, name, workspace_name=None,
                   registry_name=None, version=None, label=None):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name,
        workspace_name=workspace_name, registry_name=registry_name
    )

    try:
        model = ml_client.models.get(name=name, version=version, label=label)
        return _dump_entity_with_warnings(model)
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_model_show(
    cmd, name, workspace_name=None, resource_group_name=None, registry_name=None, version=None, label=None
):
    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx,
        resource_group_name=resource_group_name,
        workspace_name=workspace_name,
        registry_name=registry_name,
    )

    model_info = ModelInfo()

    try:
        model = ml_client.models.get(name=name, version=version, label=label)
        if model and model.type:
            model_info.model_type = model.type
        return _dump_entity_with_warnings(model)
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)
    finally:
        telemetry_log_info(model_info.__dict__)


def ml_model_package(
        cmd, name, file, version, workspace_name=None, resource_group_name=None,
        registry_name=None):
    if workspace_name and registry_name:
        workspace_reference = workspace_name
    else:
        workspace_reference = None

    ml_client, debug = get_ml_client(
        cli_ctx=cmd.cli_ctx,
        resource_group_name=resource_group_name,
        workspace_name=workspace_name,
        registry_name=registry_name,
        workspace_reference=workspace_reference,
    )

    try:
        model_pack = load_model_package(source=file)
        model = ml_client.models.package(name=name, package_request=model_pack, version=version)
        return _dump_entity_with_warnings(model)
    except Exception as err:  # pylint: disable=broad-exception-caught
        log_and_raise_error(err, debug)


def ml_model_share(cmd, name, version, share_with_name, share_with_version,
                   registry_name, resource_group_name=None, workspace_name=None):
    ml_client, _ = get_ml_client(
        cli_ctx=cmd.cli_ctx, resource_group_name=resource_group_name,
        workspace_name=workspace_name
    )

    model_info = ModelInfo()
    model_info.scenario = "promote"
    model = ml_client.models.share(name=name, version=version, registry_name=registry_name,
                                   share_with_name=share_with_name,
                                   share_with_version=share_with_version)
    return _dump_entity_with_warnings(model)
