# coding=utf-8
# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for
# license information.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------

from .tracked_resource import TrackedResource


class BigDataPoolResourceInfo(TrackedResource):
    """Big Data pool.

    A Big Data pool.

    Variables are only populated by the server, and will be ignored when
    sending a request.

    All required parameters must be populated in order to send to Azure.

    :ivar id: Fully qualified resource Id for the resource. Ex -
     /subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{resourceType}/{resourceName}
    :vartype id: str
    :ivar name: The name of the resource
    :vartype name: str
    :ivar type: The type of the resource. Ex-
     Microsoft.Compute/virtualMachines or Microsoft.Storage/storageAccounts.
    :vartype type: str
    :param tags: Resource tags.
    :type tags: dict[str, str]
    :param location: Required. The geo-location where the resource lives
    :type location: str
    :param provisioning_state: The state of the Big Data pool.
    :type provisioning_state: str
    :param auto_scale: Auto-scaling properties
    :type auto_scale: ~azure.mgmt.synapse.models.AutoScaleProperties
    :param creation_date: The time when the Big Data pool was created.
    :type creation_date: datetime
    :param auto_pause: Auto-pausing properties
    :type auto_pause: ~azure.mgmt.synapse.models.AutoPauseProperties
    :param spark_events_folder: The Spark events folder
    :type spark_events_folder: str
    :param node_count: The number of nodes in the Big Data pool.
    :type node_count: int
    :param library_requirements: Library version requirements
    :type library_requirements: ~azure.mgmt.synapse.models.LibraryRequirements
    :param spark_version: The Apache Spark version.
    :type spark_version: str
    :param default_spark_log_folder: The default folder where Spark logs will
     be written.
    :type default_spark_log_folder: str
    :param node_size: The level of compute power that each node in the Big
     Data pool has. Possible values include: 'None', 'Small', 'Medium', 'Large'
    :type node_size: str or ~azure.mgmt.synapse.models.NodeSize
    :param node_size_family: The kind of nodes that the Big Data pool
     provides. Possible values include: 'None', 'MemoryOptimized'
    :type node_size_family: str or ~azure.mgmt.synapse.models.NodeSizeFamily
    """

    _validation = {
        'id': {'readonly': True},
        'name': {'readonly': True},
        'type': {'readonly': True},
        'location': {'required': True},
        'node_count': {'maximum': 200, 'minimum': 3},
    }

    _attribute_map = {
        'id': {'key': 'id', 'type': 'str'},
        'name': {'key': 'name', 'type': 'str'},
        'type': {'key': 'type', 'type': 'str'},
        'tags': {'key': 'tags', 'type': '{str}'},
        'location': {'key': 'location', 'type': 'str'},
        'provisioning_state': {'key': 'properties.provisioningState', 'type': 'str'},
        'auto_scale': {'key': 'properties.autoScale', 'type': 'AutoScaleProperties'},
        'creation_date': {'key': 'properties.creationDate', 'type': 'iso-8601'},
        'auto_pause': {'key': 'properties.autoPause', 'type': 'AutoPauseProperties'},
        'spark_events_folder': {'key': 'properties.sparkEventsFolder', 'type': 'str'},
        'node_count': {'key': 'properties.nodeCount', 'type': 'int'},
        'library_requirements': {'key': 'properties.libraryRequirements', 'type': 'LibraryRequirements'},
        'spark_version': {'key': 'properties.sparkVersion', 'type': 'str'},
        'default_spark_log_folder': {'key': 'properties.defaultSparkLogFolder', 'type': 'str'},
        'node_size': {'key': 'properties.nodeSize', 'type': 'str'},
        'node_size_family': {'key': 'properties.nodeSizeFamily', 'type': 'str'},
    }

    def __init__(self, **kwargs):
        super(BigDataPoolResourceInfo, self).__init__(**kwargs)
        self.provisioning_state = kwargs.get('provisioning_state', None)
        self.auto_scale = kwargs.get('auto_scale', None)
        self.creation_date = kwargs.get('creation_date', None)
        self.auto_pause = kwargs.get('auto_pause', None)
        self.spark_events_folder = kwargs.get('spark_events_folder', None)
        self.node_count = kwargs.get('node_count', None)
        self.library_requirements = kwargs.get('library_requirements', None)
        self.spark_version = kwargs.get('spark_version', None)
        self.default_spark_log_folder = kwargs.get('default_spark_log_folder', None)
        self.node_size = kwargs.get('node_size', None)
        self.node_size_family = kwargs.get('node_size_family', None)
