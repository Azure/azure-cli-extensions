# --------------------------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# Licensed under the MIT License. See License.txt in the project root for
# license information.
#
# Code generated by Microsoft (R) AutoRest Code Generator.
# Changes may cause incorrect behavior and will be lost if the code is
# regenerated.
# --------------------------------------------------------------------------
# pylint: disable=wildcard-import
# pylint: disable=unused-wildcard-import
# pylint: disable=protected-access, line-too-long

from collections import defaultdict
from azure.cli.core.aaz import has_value, AAZListArg, AAZStrArg, AAZIntArg, register_command
from azure.cli.core.azclierror import ValidationError
from azure.cli.core.commands.validators import validate_file_or_dict

from .aaz.latest.monitor.data_collection.rule import Create as _RuleCreate, Update as _RuleUpdate, Show as RuleShow

try:
    from .manual.custom import *  # noqa: F403
except ImportError as e:
    if e.name.endswith('manual.custom'):
        pass
    else:
        raise e


def monitor_data_collection_rule_association_list(cmd, resource_group_name=None, data_collection_rule_name=None,
                                                  data_collection_endpoint_name=None, resource_uri=None):
    if resource_group_name and data_collection_rule_name is not None:
        from .aaz.latest.monitor.data_collection.rule.association import List as ListByRule
        return ListByRule(cli_ctx=cmd.cli_ctx)(command_args={
            "data_collection_rule_name": data_collection_rule_name,
            "resource_group": resource_group_name,
        })
    elif resource_group_name and data_collection_endpoint_name is not None:
        from .aaz.latest.monitor.data_collection.endpoint.association import List as ListByEndpoint
        return ListByEndpoint(cli_ctx=cmd.cli_ctx)(command_args={
            "data_collection_endpoint_name": data_collection_rule_name,
            "resource_group": resource_group_name,
        })
    from .aaz.latest.monitor.data_collection.rule.association import ListByResource
    return ListByResource(cli_ctx=cmd.cli_ctx)(command_args={"resource_uri": resource_uri})


class RuleCreate(_RuleCreate):

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.min_count = AAZStrArg(
            options=["--rule-file"],
            required=False,
            help='The json file for rule parameters. If provided, corresponding parameter will be overwrited by value from rule file' + '''
            Usage:   --rule-file sample.json
            rule json file should be rule parameters organized as json format, like below:
        {
            "properties": {
                "destinations": {
                    "azureMonitorMetrics": {
                        "name": "azureMonitorMetrics-default"
                    }
                },
                "dataFlows": [
                    {
                        "streams": [
                            "Microsoft-InsightsMetrics"
                        ],
                        "destinations": [
                            "azureMonitorMetrics-default"
                        ]
                    }
                ]
            }
        }.
        ''',
        )
        return args_schema

    def pre_operations(self):
        args = self.ctx.args
        if not has_value(args.rule_file):
            return
        rule_file = args.rule_file.to_serialized_data()
        from azure.cli.core.util import get_file_json
        from azure.cli.core.azclierror import FileOperationError, UnclassifiedUserFault

        try:
            json_data = get_file_json(rule_file)
        except FileNotFoundError:
            raise FileOperationError("No such file: " + str(rule_file))
        except IsADirectoryError:
            raise FileOperationError("Is a directory: " + str(rule_file))
        except PermissionError:
            raise FileOperationError("Permission denied: " + str(rule_file))
        except OSError as e:
            raise UnclassifiedUserFault(e)
        for key_prop in json_data:
            if key_prop == 'properties':
                data = json_data['properties']
            else:
                data = json_data
        for key in data:
            if key == 'dataSources':
                # args.data_sources = {}
                for key_ds in data['dataSources']:
                    if key_ds == 'performanceCounters':
                        args.data_sources.performance_counters = data['dataSources']['performanceCounters']
                    if key_ds == 'windowsEventLogs':
                        args.data_sources.windows_event_logs = data['dataSources']['windowsEventLogs']
                    if key_ds == 'syslog':
                        args.data_sources.syslog = data['dataSources']['syslog']
                    if key_ds == 'extensions':
                        args.data_sources.extensions = data['dataSources']['extensions']
            if key == 'destinations':
                # args.destinations = {}
                for key_de in data['destinations']:
                    if key_de == 'logAnalytics':
                        args.destinations.log_analytics = data['destinations']['logAnalytics']
                    if key_de == 'azureMonitorMetrics':
                        args.destinations.azure_monitor_metrics = data['destinations']['azureMonitorMetrics']
            if key == 'dataFlows':
                args.data_flows = data['dataFlows']
            if key == "streamDeclarations":
                args.stream_declarations = data["streamDeclarations"]


def process_data_flows_remain(args):
    if not has_value(args.data_flows_remain):
        return
    properties = defaultdict(list)
    for x in args.data_flows_remain:
        if has_value(x):
            if '=' in x.to_serialized_data():
                key, value = x.to_serialized_data().split('=', 1)
                properties[key].append(value)
            else:
                raise ValidationError('--data-flows format: [KEY=VALUE ...]')

    properties = dict(properties)
    data_flows = {}
    for k, v in properties.items():
        kl = k.lower()
        if kl == 'streams':
            data_flows['streams'] = v
        elif kl == 'destinations':
            data_flows['destinations'] = v
    args.data_flows.append(data_flows)


def process_data_source_extension(args):
    if not has_value(args.extensions):
        return
    extension_value = validate_file_or_dict(args.extensions.to_serialized_data())
    if extension_value is not None:
        args.data_sources.extensions.append(extension_value)


def process_data_source_performance_counters(args):
    if not has_value(args.performance_counters):
        return
    properties = defaultdict(list)
    for x in args.performance_counters:
        if has_value(x):
            if '=' in x.to_serialized_data():
                key, value = x.to_serialized_data().split('=', 1)
                properties[key].append(value)
            else:
                raise ValidationError('--performance-counters format: [KEY=VALUE ...]')
    properties = dict(properties)
    performance_counter = {}
    for k, v in properties.items():
        kl = k.lower()
        if kl == 'streams':
            performance_counter['streams'] = v
        elif kl == 'sampling-frequency':
            try:
                performance_counter['sampling_frequency_in_seconds'] = int(v[0])
            except ValueError:
                raise ValidationError('invalid sampling-frequency={}'.format(v[0]))
        elif kl == 'counter-specifiers':
            performance_counter['counter_specifiers'] = v
        elif kl == 'name':
            performance_counter['name'] = v[0]
    args.data_sources.performance_counters.append(performance_counter)


def process_data_source_syslog(args):
    if has_value(args.syslog):
        return
    properties = defaultdict(list)
    for x in args.syslog:
        if has_value(x):
            if '=' in x.to_serialized_data():
                key, value = x.to_serialized_data().split('=', 1)
                properties[key].append(value)
            else:
                raise ValidationError('--syslog format: [KEY=VALUE ...]')
    properties = dict(properties)
    syslog = {}
    for k, v in properties.items():
        kl = k.lower()
        if kl == 'streams':
            syslog['streams'] = v
        elif kl == 'facility-names':
            syslog['facility_names'] = v
        elif kl == 'log-levels':
            syslog['log_levels'] = v
        elif kl == 'name':
            syslog['name'] = v[0]
    args.data_sources.syslog.append(syslog)


def process_data_source_windows_event_logs(args):
    if has_value(args.windows_event_logs):
        return
    properties = defaultdict(list)
    for x in args.windows_event_logs:
        if has_value(x):
            if '=' in x.to_serialized_data():
                key, value = x.to_serialized_data().split('=', 1)
                properties[key].append(value)
            else:
                raise ValidationError('--windows-event-logs format: [KEY=VALUE ...]')
    properties = dict(properties)
    windows_event_logs = {}
    for k, v in properties.items():
        kl = k.lower()
        if kl == 'streams':
            windows_event_logs['streams'] = v
        elif kl == 'x-path-queries':
            windows_event_logs['x_path_queries'] = v
        elif kl == 'name':
            windows_event_logs['name'] = v[0]
    args.data_sources.windows_event_logs.append(windows_event_logs)


def process_destination_log_analytics(args):
    if not has_value(args.log_analytics):
        return
    properties = defaultdict(list)
    for x in args.log_analytics:
        if has_value(x):
            if '=' in x.to_serialized_data():
                key, value = x.to_serialized_data().split('=', 1)
                properties[key].append(value)
            else:
                raise ValidationError('--log-analytics format: [KEY=VALUE ...]')
    properties = dict(properties)
    log_analytics = {}
    for k, v in properties.items():
        kl = k.lower()
        if kl == 'resource-id':
            log_analytics['workspace_resource_id'] = v[0]
        elif kl == 'name':
            log_analytics['name'] = v[0]
    args.destinations.log_analytics.append(log_analytics)


def process_destination_monitor_metrics(args):
    if not has_value(args.monitor_metrics):
        return
    properties = defaultdict(list)
    for x in args.monitor_metrics:
        if has_value(x):
            if '=' in x.to_serialized_data():
                key, value = x.to_serialized_data().split('=', 1)
                properties[key].append(value)
            else:
                raise ValidationError('--monitor-metrics format: [KEY=VALUE ...]')
    properties = dict(properties)
    azure_monitor_metrics = {}
    for k, v in properties.items():
        kl = k.lower()
        if kl == 'name':
            azure_monitor_metrics['name'] = v[0]
    args.destinations.azure_monitor_metrics = azure_monitor_metrics


class RuleUpdate(_RuleUpdate):

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.data_flows_remain = AAZListArg(
            options=["--data-flows", "--data-flows-remain"],
            arg_group="Data Flow",
            help="The specification of data flows." + '''
        Usage: --data-flows streams=XX1 streams=XX2 destinations=XX1 destinations=XX2
        streams: Required. List of streams for this data flow.
        destinations: Required. List of destinations for this data flow.
        
        Multiple actions can be specified by using more than one --data-flows argument.
        '''
        )
        args_schema.data_flows_remain.Element = AAZStrArg()

        args_schema.extensions = AAZStrArg(
            options=["--extensions"],
            arg_group="Data Sources",
            help="The list of Azure VM extension data source configurations. Expected value: json-string/@json-file.",
        )

        args_schema.performance_counters = AAZListArg(
            options=["--performance-counters"],
            arg_group="Data Sources",
            help="The list of performance counter data source configurations." + '''
            Usage: --performance-counters streams=XX1 streams=XX2 sampling-frequency=XX counter-specifiers=XX1 counter-specifiers=XX2 name=XX
            streams: Required. List of streams that this data source will be sent to. A stream indicates what schema will be used for this data and usually what table in Log Analytics the data will be sent to.
            sampling-frequency: Required. The number of seconds between consecutive counter measurements(samples).
            counter-specifiers: Required. A list of specifier names of the performance counters you want to collect. Use a wildcard (*) to collect a counter for all instances. To get a list of performance counters on Windows, run the command 'typeperf'.
            name: Required. A friendly name for the data source.  This name should be unique across all data sources (regardless of type) within the data collection rule.
            Multiple actions can be specified by using more than one --performance-counters argument.
            ''',
        )
        args_schema.performance_counters.Element = AAZStrArg()

        args_schema.syslog = AAZListArg(
            options=["--syslog"],
            arg_group="Data Sources",
            help="The list of Syslog data source configurations." + '''
            Usage: --syslog streams=XX1 streams=XX2 facility-names=XX1 facility-names=XX2 log-levels=XX1 log-levels=XX2 name=XX
            streams: Required. List of streams that this data source will be sent to. A stream indicates what schema will be used for this data and usually what table in Log Analytics the data will be sent to.
            facility-names: Required. The list of facility names.
            log-levels: The log levels to collect.
            name: Required. A friendly name for the data source.  This name should be unique across all data sources (regardless of type) within the data collection rule.
            Multiple actions can be specified by using more than one --syslog argument.
            '''
        )
        args_schema.syslog.Element = AAZStrArg()

        args_schema.windows_event_logs = AAZListArg(
            options=["--windows-event-logs"],
            arg_group="Data Sources",
            help="The list of Windows Event Log data source configurations." + '''
            Usage: --windows-event-logs streams=XX1 streams=XX2 x-path-queries=XX1 x-path-queries=XX2 name=XX
            streams: Required. List of streams that this data source will be sent to. A stream indicates what schema will be used for this data and usually what table in Log Analytics the data will be sent to.
            x-path-queries: Required. A list of Windows Event Log queries in XPATH format.
            name: Required. A friendly name for the data source.  This name should be unique across all data sources (regardless of type) within the data collection rule.
            Multiple actions can be specified by using more than one --windows-event-logs argument.
            '''
        )
        args_schema.windows_event_logs.Element = AAZStrArg()

        args_schema.log_analytics = AAZListArg(
            options=["--log-analytics"],
            arg_group="Destinations",
            help="List of Log Analytics destinations." + '''
            Usage: --log-analytics resource-id=XX name=XX
            resource-id: Required. The resource ID of the Log Analytics workspace.
            name: Required. A friendly name for the destination.  This name should be unique across all destinations (regardless of type) within the data collection rule.
            Multiple actions can be specified by using more than one --log-analytics argument.
            '''
        )
        args_schema.log_analytics.Element = AAZStrArg()

        args_schema.monitor_metrics = AAZListArg(
            options=["--monitor-metrics"],
            arg_group="Destinations",
            help="Azure Monitor Metrics destination." + '''
            Usage: --monitor-metrics name=XX
            name: Required. A friendly name for the destination.  This name should be unique across all destinations (regardless of type) within the data collection rule.
            '''
        )
        args_schema.monitor_metrics.Element = AAZStrArg()

        return args_schema

    def pre_operations(self):
        args = self.ctx.args
        process_data_flows_remain(args)
        process_data_source_extension(args)
        process_data_source_performance_counters(args)
        process_data_source_syslog(args)
        process_data_source_windows_event_logs(args)
        process_destination_log_analytics(args)
        process_destination_monitor_metrics(args)


def data_collection_rules_data_flows_list(cmd, resource_group_name, data_collection_rule_name):
    rule_instance = RuleShow(cli_ctx=cmd.cli_ctx)(command_args={
        "resource_group": resource_group_name,
        "data_collection_rule_name": data_collection_rule_name})
    return rule_instance["data_flows"]


@register_command("monitor data-collection rule data-flow add")
class RuleDataFlowsAdd(_RuleUpdate):
    """Add a data flow.

    :example: Add a data flow
    az monitor data-collection rule data-flow add --rule-name "myCollectionRule"
    --resource-group "myResourceGroup" --destinations XX3 XX4 --streams "Microsoft-Perf" "Microsoft-WindowsEvent"
    """

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.data_flows_destinations = AAZListArg(
            options=["--destinations"],
            required=True,
            help="List of destinations for this data flow.",
        )
        args_schema.data_flows_destinations.Element = AAZStrArg()
        args_schema.data_flows_streams = AAZListArg(
            options=["--streams"],
            required=True,
            help="List of streams for this data flow. Values can be: Microsoft-Event, Microsoft-InsightsMetrics, Microsoft-Perf, Microsoft-Syslog, Microsoft-WindowsEvent, etc",
        )
        args_schema.data_flows_streams.Element = AAZStrArg()
        return args_schema

    def pre_instance_update(self, instance):
        args = self.ctx.args
        data_flow = {}
        if has_value(args.data_flows_streams):
            data_flow["streams"] = args.data_flows_streams.to_serialized_data()
        if has_value(args.data_flows_destinations):
            data_flow["destinations"] = args.data_flows_destinations.to_serialized_data()
        instance.properties.data_flows.append(data_flow)


def data_collection_rules_log_analytics_list(cmd, resource_group_name, data_collection_rule_name):
    rule_instance = RuleShow(cli_ctx=cmd.cli_ctx)(command_args={
        "resource_group": resource_group_name,
        "data_collection_rule_name": data_collection_rule_name
    })
    return rule_instance["destinations"]["log_analytics"]


def data_collection_rules_log_analytics_show(cmd, resource_group_name, data_collection_rule_name, name):
    rule_instance = RuleShow(cli_ctx=cmd.cli_ctx)(command_args={
        "resource_group": resource_group_name,
        "data_collection_rule_name": data_collection_rule_name
    })
    item_list = rule_instance["destinations"]["log_analytics"]
    for item in item_list:
        if item["name"] == name:
            return item
    return {}


@register_command("monitor data-collection rule log-analytics add")
class RuleDestinationsLogAnalyticsAdd(_RuleUpdate):
    """Add Log Analytics destinations of a data collection rule.

    :example: Add Log Analytics destinations of a data collection rule
    az monitor data-collection rule log-analytics add --rule-name "myCollectionRule"
    --resource-group "myResourceGroup" --name "workspace2" --resource-id "/subscriptions/703362b3-f278-4e4b-9179-c76eaf41ffc2/resourceGroups/myResourceGroup/providers/Microsoft.OperationalInsights/workspaces/workspace2"
    """

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.destination_log_analytics_name = AAZStrArg(
            options=["--name", "-n"],
            required=True,
            help="A friendly name for the destination. This name should be unique across all destinations (regardless of type) within the data collection rule.",
        )
        args_schema.destination_log_analytics_resource_id = AAZStrArg(
            options=["--resource-id"],
            required=True,
            help="The resource ID of the Log Analytics workspace.",
        )
        return args_schema

    def pre_instance_update(self, instance):
        args = self.ctx.args
        item_list = instance.properties.destinations.log_analytics
        name = args.destination_log_analytics_name.to_serialized_data()
        for item in item_list:
            if item.name == name:
                from azure.cli.core.azclierror import InvalidArgumentValueError
                raise InvalidArgumentValueError("Name {} exists.".format(name))
        log_analytic = {
            'name': name,
            'workspace_resource_id': args.destination_log_analytics_resource_id.to_serialized_data()
        }
        instance.properties.destinations.log_analytics.append(log_analytic)


@register_command("monitor data-collection rule log-analytics delete")
class RuleDestinationsLogAnalyticsDelete(_RuleUpdate):
    """Delete a Log Analytics destinations of a data collection rule.

    :example: Delete a Log Analytics destinations of a data collection rule
    az monitor data-collection rule log-analytics delete --rule-name "myCollectionRule"
    --resource-group "myResourceGroup" --name "workspace2"
    """

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.destination_log_analytics_name = AAZStrArg(
            options=["--name", "-n"],
            required=True,
            help="A friendly name for the destination. This name should be unique across all destinations (regardless of type) within the data collection rule.",
        )

        return args_schema

    def pre_instance_update(self, instance):
        args = self.ctx.args
        item_list = instance.properties.destinations.log_analytics
        name = args.destination_log_analytics_name.to_serialized_data()
        remain_list = []
        for item in item_list:
            if item.name != name:
                remain_list.append(item)
        instance.properties.destinations.log_analytics = remain_list


@register_command("monitor data-collection rule log-analytics update")
class RuleDestinationsLogAnalyticsUpdate(_RuleUpdate):
    """Update a Log Analytics destination of a data collection rule.

    :example: Update a Log Analytics destination of a data collection rule
    az monitor data-collection rule log-analytics update --rule-name "myCollectionRule"
    --resource-group "myResourceGroup" --name "workspace2"
    --resource-id "/subscriptions/703362b3-f278-4e4b-9179-c76eaf41ffc2/resourceGroups/myResourceGroup/providers/Microsoft.OperationalInsights/workspaces/anotherWorkspace"
    """

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.destination_log_analytics_name = AAZStrArg(
            options=["--name", "-n"],
            required=True,
            help="A friendly name for the destination. This name should be unique across all destinations (regardless of type) within the data collection rule.",
        )
        args_schema.destination_log_analytics_resource_id = AAZStrArg(
            options=["--resource-id"],
            help="The resource ID of the Log Analytics workspace.",
        )
        return args_schema

    def pre_instance_update(self, instance):
        args = self.ctx.args
        if not has_value(args.destination_log_analytics_resource_id):
            return
        item_list = instance.properties.destinations.log_analytics
        name = args.destination_log_analytics_name.to_serialized_data()
        for item in item_list:
            if item.name == name:
                item.workspace_resource_id = args.destination_log_analytics_resource_id.to_serialized_data()
                break


def data_collection_rules_performance_counters_list(cmd, resource_group_name, data_collection_rule_name):
    rule_instance = RuleShow(cli_ctx=cmd.cli_ctx)(command_args={
        "resource_group": resource_group_name,
        "data_collection_rule_name": data_collection_rule_name
    })
    return rule_instance["data_sources"]["performance_counters"]


def data_collection_rules_performance_counters_show(cmd, resource_group_name, data_collection_rule_name, name):
    rule_instance = RuleShow(cli_ctx=cmd.cli_ctx)(command_args={
        "resource_group": resource_group_name,
        "data_collection_rule_name": data_collection_rule_name
    })
    item_list = rule_instance["data_sources"]["performance_counters"]
    for item in item_list:
        if item['name'] == name:
            return item
    return {}


@register_command("monitor data-collection rule performance-counter add")
class RuleDataSourcesPerformanceCountersAdd(_RuleUpdate):
    """Add a Log performance counter data source.

    :example: Add a Log performance counter data source
    az monitor data-collection rule performance-counter add --rule-name "myCollectionRule"
    --resource-group "myResourceGroup" --name "team2ExtraCounters" --streams "Microsoft-Perf"
    --counter-specifiers "\\Process(_Total)\\Thread Count" "\\LogicalDisk(_Total)\\Free
    Megabytes" --sampling-frequency 30
    """

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.performance_counters_name = AAZStrArg(
            options=["--name", "-n"],
            required=True,
            help="A friendly name for the data source. "
                 "This name should be unique across all data sources (regardless of type) within the data collection rule.",
        )
        args_schema.performance_counters_streams = AAZListArg(
            options=["--streams"],
            required=True,
            help="List of streams that this data source will be sent to. "
                 "A stream indicates what schema will be used for this data and usually what table in Log Analytics the data will be sent to. "
                 "Values example: Microsoft-InsightsMetrics, Microsoft-Perf.",
        )
        args_schema.performance_counters_streams.Element = AAZStrArg()

        args_schema.performance_counters_sampling_frequency_in_seconds = AAZIntArg(
            options=["--sampling-frequency"],
            required=True,
            help="The number of seconds between consecutive counter measurements (samples).",
        )

        args_schema.performance_counters_counter_specifiers = AAZListArg(
            options=["--counter-specifiers"],
            required=True,
            help="A list of specifier names of the performance counters you want to collect. "
                 "Use a wildcard (*) to collect a counter for all instances. "
                 "To get a list of performance counters on Windows, run the command 'typeperf'.",
        )
        args_schema.performance_counters_counter_specifiers.Element = AAZStrArg()

        return args_schema

    def pre_instance_update(self, instance):
        args = self.ctx.args
        item_list = instance.properties.data_sources.performance_counters
        name = args.performance_counters_name.to_serialized_data()
        for item in item_list:
            if item.name == name:
                from azure.cli.core.azclierror import InvalidArgumentValueError
                raise InvalidArgumentValueError("Name {} exists.".format(name))

        performance_counter = {
            'name': name,
            'streams': args.performance_counters_streams.to_serialized_data(),
            'sampling_frequency_in_seconds': args.performance_counters_sampling_frequency_in_seconds.to_serialized_data(),
            'counter_specifiers': args.performance_counters_counter_specifiers.to_serialized_data()
        }
        instance.properties.data_sources.performance_counters.append(performance_counter)


@register_command("monitor data-collection rule performance-counter delete")
class RuleDataSourcesPerformanceCountersDelete(_RuleUpdate):
    """Delete a Log performance counter data source.

    :example: Delete a Log performance counter data source
    az monitor data-collection rule performance-counter delete --rule-name "myCollectionRule"
    --resource-group "myResourceGroup" --name "team2ExtraCounters"
    """

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.performance_counters_name = AAZStrArg(
            options=["--name", "-n"],
            required=True,
            help="A friendly name for the data source. "
                 "This name should be unique across all data sources (regardless of type) within the data collection rule.",
        )
        return args_schema

    def pre_instance_update(self, instance):
        args = self.ctx.args
        item_list = instance.properties.data_sources.performance_counters
        name = args.performance_counters_name.to_serialized_data()
        remained_items = []
        for item in item_list:
            if item.name != name:
                remained_items.append(item)

        instance.properties.data_sources.performance_counters = remained_items


@register_command("monitor data-collection rule performance-counter update")
class RuleDataSourcesPerformanceCountersUpdate(_RuleUpdate):
    """Update a Log performance counter data source.

        :example: Update a Log performance counter data source
            az monitor data-collection rule performance-counter update --rule-name "myCollectionRule"
            --resource-group "myResourceGroup" --name "team2ExtraCounters"
    """

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.performance_counters_name = AAZStrArg(
            options=["--name", "-n"],
            required=True,
            help="A friendly name for the data source. "
                 "This name should be unique across all data sources (regardless of type) within the data collection rule.",
        )
        args_schema.performance_counters_streams = AAZListArg(
            options=["--streams"],
            required=True,
            help="List of streams that this data source will be sent to. "
                 "A stream indicates what schema will be used for this data and usually what table in Log Analytics the data will be sent to. "
                 "Values example: Microsoft-InsightsMetrics, Microsoft-Perf.",
        )
        args_schema.performance_counters_streams.Element = AAZStrArg()

        args_schema.performance_counters_sampling_frequency_in_seconds = AAZIntArg(
            options=["--sampling-frequency"],
            required=True,
            help="The number of seconds between consecutive counter measurements (samples).",
        )

        args_schema.performance_counters_counter_specifiers = AAZListArg(
            options=["--counter-specifiers"],
            required=True,
            help="A list of specifier names of the performance counters you want to collect. "
                 "Use a wildcard (*) to collect a counter for all instances. "
                 "To get a list of performance counters on Windows, run the command 'typeperf'.",
        )
        args_schema.performance_counters_counter_specifiers.Element = AAZStrArg()

        return args_schema

    def pre_instance_update(self, instance):
        args = self.ctx.args
        item_list = instance.properties.data_sources.performance_counters
        name = args.performance_counters_name.to_serialized_data()
        for item in item_list:
            if item.name == name:
                if has_value(args.performance_counters_streams):
                    item.streams = args.performance_counters_streams.to_serialized_data()
                if has_value(args.performance_counters_sampling_frequency_in_seconds):
                    item.sampling_frequency_in_seconds = args.performance_counters_sampling_frequency_in_seconds.to_serialized_data()
                if has_value(args.performance_counters_counter_specifiers):
                    item.counter_specifiers = args.performance_counters_counter_specifiers.to_serialized_data()
                break


def data_collection_rules_windows_event_logs_list(cmd, resource_group_name, data_collection_rule_name):
    rule_instance = RuleShow(cli_ctx=cmd.cli_ctx)(command_args={
        "resource_group": resource_group_name,
        "data_collection_rule_name": data_collection_rule_name
    })
    return rule_instance["data_sources"]["windows_event_logs"]


def data_collection_rules_windows_event_logs_show(cmd, resource_group_name, data_collection_rule_name, name):
    rule_instance = RuleShow(cli_ctx=cmd.cli_ctx)(command_args={
        "resource_group": resource_group_name,
        "data_collection_rule_name": data_collection_rule_name
    })
    item_list = rule_instance["data_sources"]["windows_event_logs"]
    for item in item_list:
        if item['name'] == name:
            return item
    return {}


@register_command("monitor data-collection rule windows-event-log add")
class RuleDataSourcesWindowsEventLogsAdd(_RuleUpdate):
    """Add a Windows Event Log data source.

    :example: Add a Windows Event Log data source
    az monitor data-collection rule windows-event-log add --rule-name "myCollectionRule"
    --resource-group "myResourceGroup" --name "appTeam1AppEvents" --streams "Microsoft-WindowsEvent"
    --x-path-queries "Application!*[System[(Level = 1 or Level = 2 or Level =3)]]" "System![System[(Level = 1 or Level = 2 or Level = 3)]]"
    """

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.windows_event_logs_name = AAZStrArg(
            options=["--name", "-n"],
            required=True,
            help="A friendly name for the data source. "
                 "This name should be unique across all data sources (regardless of type) within the data collection rule.",
        )
        args_schema.windows_event_logs_streams = AAZListArg(
            options=["--streams"],
            required=True,
            help="List of streams that this data source will be sent to. "
                 "A stream indicates what schema will be used for this data and usually what table in Log Analytics the data will be sent to.  "
                 "Example values: Microsoft-Event, Microsoft-WindowsEvent.",
        )
        args_schema.windows_event_logs_streams.Element = AAZStrArg()

        args_schema.windows_event_logs_x_path_queries = AAZListArg(
            options=["--x-path-queries"],
            required=True,
            help="A list of Windows Event Log queries in XPATH format.",
        )
        args_schema.windows_event_logs_x_path_queries.Element = AAZStrArg()

        return args_schema

    def pre_instance_update(self, instance):
        args = self.ctx.args
        item_list = instance.properties.data_sources.windows_event_logs
        name = args.windows_event_logs_name.to_serialized_data()
        for item in item_list:
            if item.name == name:
                from azure.cli.core.azclierror import InvalidArgumentValueError
                raise InvalidArgumentValueError("Name {} exists.".format(name))

        windows_event_log = {
            'name': name,
            'streams': args.windows_event_logs_streams.to_serialized_data(),
            'x_path_queries': args.windows_event_logs_x_path_queries.to_serialized_data()
        }
        instance.properties.data_sources.windows_event_logs.append(windows_event_log)


@register_command("monitor data-collection rule windows-event-log delete")
class RuleDataSourcesWindowsEventLogsDelete(_RuleUpdate):
    """Delete a Windows Event Log data source.

    :example: Delete a Windows Event Log data source
    az monitor data-collection rule windows-event-log delete --rule-name "myCollectionRule"
    --resource-group "myResourceGroup" --name "appTeam1AppEvents"
    """

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.windows_event_logs_name = AAZStrArg(
            options=["--name", "-n"],
            required=True,
            help="A friendly name for the data source. "
                 "This name should be unique across all data sources (regardless of type) within the data collection rule.",
        )
        return args_schema

    def pre_instance_update(self, instance):
        args = self.ctx.args
        item_list = instance.properties.data_sources.windows_event_logs
        name = args.windows_event_logs_name.to_serialized_data()
        remained_list = []
        for item in item_list:
            if item.name != name:
                remained_list.append(item)

        instance.properties.data_sources.windows_event_logs = remained_list


@register_command("monitor data-collection rule windows-event-log update")
class RuleDataSourcesWindowsEventLogsUpdate(_RuleUpdate):
    """Update a Windows Event Log data source.

    :example: Update a Windows Event Log data source
    az monitor data-collection rule windows-event-log update --rule-name "myCollectionRule"
    --resource-group "myResourceGroup" --name "appTeam1AppEvents"
    --x-path-queries "Application!*[System[(Level = 1 or Level = 2 or Level = 3)]]"
    """

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.windows_event_logs_name = AAZStrArg(
            options=["--name", "-n"],
            required=True,
            help="A friendly name for the data source. "
                 "This name should be unique across all data sources (regardless of type) within the data collection rule.",
        )
        args_schema.windows_event_logs_streams = AAZListArg(
            options=["--streams"],
            help="List of streams that this data source will be sent to. "
                 "A stream indicates what schema will be used for this data and usually what table in Log Analytics the data will be sent to.  "
                 "Example values: Microsoft-Event, Microsoft-WindowsEvent.",
        )
        args_schema.windows_event_logs_streams.Element = AAZStrArg()

        args_schema.windows_event_logs_x_path_queries = AAZListArg(
            options=["--x-path-queries"],
            help="A list of Windows Event Log queries in XPATH format.",
        )
        args_schema.windows_event_logs_x_path_queries.Element = AAZStrArg()

        return args_schema

    def pre_instance_update(self, instance):
        args = self.ctx.args
        item_list = instance.properties.data_sources.windows_event_logs
        name = args.windows_event_logs_name.to_serialized_data()
        for item in item_list:
            if item.name == name:
                if has_value(args.windows_event_logs_streams):
                    item.streams = args.windows_event_logs_streams.to_serialized_data()
                if has_value(args.windows_event_logs_x_path_queries):
                    item.x_path_queries = args.windows_event_logs_x_path_queries.to_serialized_data()
                break


def data_collection_rules_syslog_list(cmd, resource_group_name, data_collection_rule_name):
    rule_instance = RuleShow(cli_ctx=cmd.cli_ctx)(command_args={
        "resource_group": resource_group_name,
        "data_collection_rule_name": data_collection_rule_name
    })
    return rule_instance["data_sources"]["syslog"]


def data_collection_rules_syslog_show(cmd, resource_group_name, data_collection_rule_name, name):
    rule_instance = RuleShow(cli_ctx=cmd.cli_ctx)(command_args={
        "resource_group": resource_group_name,
        "data_collection_rule_name": data_collection_rule_name
    })
    item_list = rule_instance["data_sources"]["syslog"]
    for item in item_list:
        if item['name'] == name:
            return item
    return {}


@register_command("monitor data-collection rule syslog add")
class RuleDataSourcesSyslogAdd(_RuleUpdate):
    """Add a Syslog data source.

    :example: Add a Syslog data source
    az monitor data-collection rule syslog add --rule-name "myCollectionRule" --resource-group
    "myResourceGroup" --name "syslogBase" --facility-names "syslog" --log-levels "Alert" "Critical"
    --streams "Microsoft-Syslog"
    """

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.syslog_name = AAZStrArg(
            options=["--name", "-n"],
            required=True,
            help="A friendly name for the data source. "
                 "This name should be unique across all data sources (regardless of type) within the data collection rule.",
        )
        args_schema.syslog_streams = AAZListArg(
            options=["--streams"],
            required=True,
            help="List of streams that this data source will be sent to. "
                 "A stream indicates what schema will be used for this data and usually what table in Log Analytics the data will be sent to.  "
                 "Example values: Microsoft-Syslog.",
        )
        args_schema.syslog_streams.Element = AAZStrArg()

        args_schema.syslog_facility_names = AAZListArg(
            options=["--facility-names"],
            required=True,
            help="The list of facility names.  "
                 "Example values: *, auth, authpriv, cron, daemon, kern, local0, local1, local2, local3, local4, local5, local6, local7, "
                 "lpr, mail, mark, news, syslog, user, uucp.",
        )
        args_schema.syslog_facility_names.Element = AAZStrArg()

        args_schema.syslog_log_levels = AAZListArg(
            options=["--log-levels"],
            help="The log levels to collect.  Allowed values: *, Alert, Critical, Debug, Emergency, Error, Info, Notice, Warning.",
        )
        args_schema.syslog_log_levels.Element = AAZStrArg()

        return args_schema

    def pre_instance_update(self, instance):
        args = self.ctx.args
        item_list = instance.properties.data_sources.syslog
        name = args.syslog_name.to_serialized_data()
        for item in item_list:
            if item.name == name:
                from azure.cli.core.azclierror import InvalidArgumentValueError
                raise InvalidArgumentValueError("Name {} exists.".format(name))

        syslog = {
            'name': name,
            'streams': args.syslog_streams.to_serialized_data(),
            'facility_names': args.syslog_facility_names.to_serialized_data()
        }
        if has_value(args.syslog_log_levels):
            syslog["log_levels"] = args.syslog_log_levels.to_serialized_data()
        instance.properties.data_sources.syslog.append(syslog)


@register_command("monitor data-collection rule syslog delete")
class RuleDataSourcesSyslogDelete(_RuleUpdate):
    """Delete a Syslog data source.

    :example: Delete a Syslog data source
    az monitor data-collection rule syslog delete --rule-name "myCollectionRule"
    --resource-group "myResourceGroup" --name "syslogBase"
    """

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.syslog_name = AAZStrArg(
            options=["--name", "-n"],
            required=True,
            help="A friendly name for the data source. "
                 "This name should be unique across all data sources (regardless of type) within the data collection rule.",
        )
        return args_schema

    def pre_instance_update(self, instance):
        args = self.ctx.args
        item_list = instance.properties.data_sources.syslog
        name = args.syslog_name.to_serialized_data()
        remained_list = []
        for item in item_list:
            if item.name != name:
                remained_list.append(item)
        instance.properties.data_sources.syslog = remained_list


@register_command("monitor data-collection rule syslog update")
class RuleDataSourcesSyslogUpdate(_RuleUpdate):
    """Update a Syslog data source.

    :example: Update a Syslog data source
    az monitor data-collection rule syslog update --rule-name "myCollectionRule"
    --resource-group "myResourceGroup" --name "syslogBase" --facility-names "syslog" --log-levels "Emergency" "Critical"
    """

    @classmethod
    def _build_arguments_schema(cls, *args, **kwargs):
        args_schema = super()._build_arguments_schema(*args, **kwargs)
        args_schema.syslog_name = AAZStrArg(
            options=["--name", "-n"],
            required=True,
            help="A friendly name for the data source. "
                 "This name should be unique across all data sources (regardless of type) within the data collection rule.",
        )
        args_schema.syslog_streams = AAZListArg(
            options=["--streams"],
            help="List of streams that this data source will be sent to. "
                 "A stream indicates what schema will be used for this data and usually what table in Log Analytics the data will be sent to.  "
                 "Example values: Microsoft-Syslog.",
        )
        args_schema.syslog_streams.Element = AAZStrArg()

        args_schema.syslog_facility_names = AAZListArg(
            options=["--facility-names"],
            help="The list of facility names.  "
                 "Example values: *, auth, authpriv, cron, daemon, kern, local0, local1, local2, local3, local4, local5, local6, local7, "
                 "lpr, mail, mark, news, syslog, user, uucp.",
        )
        args_schema.syslog_facility_names.Element = AAZStrArg()

        args_schema.syslog_log_levels = AAZListArg(
            options=["--log-levels"],
            help="The log levels to collect.  Allowed values: *, Alert, Critical, Debug, Emergency, Error, Info, Notice, Warning.",
        )
        args_schema.syslog_log_levels.Element = AAZStrArg()

        return args_schema

    def pre_instance_update(self, instance):
        args = self.ctx.args
        item_list = instance.properties.data_sources.syslog
        name = args.syslog_name.to_serialized_data()
        for item in item_list:
            if item.name == name:
                if has_value(args.syslog_streams):
                    item.streams = args.syslog_streams.to_serialized_data()
                if has_value(args.syslog_facility_names):
                    item.facility_names = args.syslog_facility_names.to_serialized_data()
                if has_value(args.syslog_log_levels):
                    item.log_levels = args.syslog_log_levels.to_serialized_data()
                break
